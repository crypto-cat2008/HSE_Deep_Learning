{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of week05_part_of_speech_tagging.ipynb","provenance":[{"file_id":"https://github.com/hse-aml/intro-to-dl-pytorch/blob/main/week05/week05_part_of_speech_tagging.ipynb","timestamp":1655327387310}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"collapsed":true,"id":"WwNfzDcnXJoN"},"source":["## Part Of Speech Tagging\n","\n","<img src=https://i.stack.imgur.com/6pdIT.png width=320>\n","\n","Unlike our previous experience with language modelling, this time around we learn the mapping between two different kinds of elements.\n","\n","This setting is common for a range of useful problems:\n","* Speech Recognition - processing human voice into text\n","* Part Of Speech Tagging - for morphology-aware search and as an auxuliary task for most NLP problems\n","* Named Entity Recognition - for chat bots and web crawlers\n","* Protein structure prediction - for bioinformatics\n","\n","In this programming assignment we will work with part-of-speech tagging. As the name suggests, it's about converting a sequence of words into a sequence of part-of-speech tags. We'll use a reduced tag set for simplicity:\n","\n","### POS-tags\n","- ADJ - adjective (new, good, high, ...)\n","- ADP - adposition\t(on, of, at, ...)\n","- ADV - adverb\t(really, already, still, ...)\n","- CONJ\t- conjunction\t(and, or, but, ...)\n","- DET - determiner, article\t(the, a, some, ...)\n","- NOUN\t- noun\t(year, home, costs, ...)\n","- NUM - numeral\t(twenty-four, fourth, 1991, ...)\n","- PRT -\tparticle (at, on, out, ...)\n","- PRON - pronoun (he, their, her, ...)\n","- VERB - verb (is, say, told, ...)\n","- .\t- punctuation marks\t(. , ;)\n","- X\t- other\t(ersatz, esprit, dunno, ...)\n","\n","__Disclaimer:__ This assignment is ungraded."]},{"cell_type":"code","metadata":{"id":"zTG0uxEbXJoP","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655327653465,"user_tz":240,"elapsed":8508,"user":{"displayName":"Susan Malkin","userId":"07747390338629969515"}},"outputId":"0f8af830-c4be-4033-a01c-7cb6f8823720"},"source":["import nltk\n","import sys\n","import numpy as np\n","\n","nltk.download('brown')\n","nltk.download('universal_tagset')\n","\n","data = nltk.corpus.brown.tagged_sents(tagset='universal')\n","all_tags = ['#EOS#','#UNK#','ADV', 'NOUN', 'ADP', 'PRON', 'DET', '.', 'PRT', 'VERB', 'X', 'NUM', 'CONJ', 'ADJ']\n","\n","data = np.array([[(word.lower(),tag) for word, tag in sentence] for sentence in data])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Unzipping taggers/universal_tagset.zip.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  # This is added back by InteractiveShellApp.init_path()\n"]}]},{"cell_type":"code","metadata":{"id":"8jRfjw3FXJoQ"},"source":["from sklearn.model_selection import train_test_split\n","train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L2AHSrM1XJoR","colab":{"base_uri":"https://localhost:8080/","height":152},"executionInfo":{"status":"ok","timestamp":1655327664922,"user_tz":240,"elapsed":238,"user":{"displayName":"Susan Malkin","userId":"07747390338629969515"}},"outputId":"59997f77-2955-4ca7-d2d8-8d797726d8d6"},"source":["from IPython.display import HTML, display\n","def draw(sentence):\n","    words,tags = zip(*sentence)\n","    display(HTML('<table><tr>{tags}</tr>{words}<tr></table>'.format(\n","                words = '<td>{}</td>'.format('</td><td>'.join(words)),\n","                tags = '<td>{}</td>'.format('</td><td>'.join(tags)))))\n","    \n","    \n","draw(data[11])\n","draw(data[10])\n","draw(data[7])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table><tr><td>NOUN</td><td>ADP</td><td>NOUN</td><td>NOUN</td><td>NOUN</td><td>NOUN</td><td>VERB</td><td>ADV</td><td>VERB</td><td>ADP</td><td>DET</td><td>ADJ</td><td>NOUN</td><td>.</td></tr><td>implementation</td><td>of</td><td>georgia's</td><td>automobile</td><td>title</td><td>law</td><td>was</td><td>also</td><td>recommended</td><td>by</td><td>the</td><td>outgoing</td><td>jury</td><td>.</td><tr></table>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table><tr><td>PRON</td><td>VERB</td><td>ADP</td><td>DET</td><td>NOUN</td><td>.</td><td>VERB</td><td>NOUN</td><td>PRT</td><td>VERB</td><td>.</td><td>DET</td><td>NOUN</td><td>.</td></tr><td>it</td><td>urged</td><td>that</td><td>the</td><td>city</td><td>``</td><td>take</td><td>steps</td><td>to</td><td>remedy</td><td>''</td><td>this</td><td>problem</td><td>.</td><tr></table>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table><tr><td>NOUN</td><td>VERB</td></tr><td>merger</td><td>proposed</td><tr></table>"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"SbwpLW0mXJoS"},"source":["### Building vocabularies\n","\n","Just like before, we have to build a mapping from tokens to integer ids. This time around, our model operates on a word level, processing one word per RNN step. This means we'll have to deal with far larger vocabulary.\n","\n","Luckily for us, we only receive those words as input i.e. we don't have to predict them. This means we can have a large vocabulary for free by using word embeddings."]},{"cell_type":"code","metadata":{"id":"9qOGuB1xXJoS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655327758603,"user_tz":240,"elapsed":613,"user":{"displayName":"Susan Malkin","userId":"07747390338629969515"}},"outputId":"fde7bcea-c530-4eec-9d1c-d8e6da0a6df4"},"source":["from collections import Counter\n","word_counts = Counter()\n","for sentence in data:\n","    words,tags = zip(*sentence)\n","    word_counts.update(words)\n","\n","all_words = ['#EOS#','#UNK#'] + list(list(zip(*word_counts.most_common(10000)))[0])\n","\n","#let's measure what fraction of data words are in the dictionary\n","print(\"Coverage = %.5f\"%(float(sum(word_counts[w] for w in all_words)) / sum(word_counts.values())))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Coverage = 0.92876\n"]}]},{"cell_type":"code","metadata":{"id":"kCNS_olOXJoT"},"source":["from collections import defaultdict\n","word_to_id = defaultdict(lambda: 1, {word: i for i, word in enumerate(all_words)})\n","tag_to_id = {tag:i for i, tag in enumerate(all_tags)}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FHs7kEoqXJoU"},"source":["convert words and tags into fixed-size matrix"]},{"cell_type":"code","metadata":{"id":"b58RLWShXJoU"},"source":["def to_matrix(lines, token_to_id, max_len=None, pad=0, dtype='int32', time_major=False):\n","    \"\"\"Converts a list of names into rnn-digestable matrix with paddings added after the end\"\"\"\n","    \n","    max_len = max_len or max(map(len,lines))\n","    matrix = np.empty([len(lines),max_len],dtype)\n","    matrix.fill(pad)\n","\n","    for i in range(len(lines)):\n","        line_ix = list(map(token_to_id.__getitem__,lines[i]))[:max_len]\n","        matrix[i,:len(line_ix)] = line_ix\n","\n","    return matrix.T if time_major else matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pCxwnAbPXJoV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655327808280,"user_tz":240,"elapsed":309,"user":{"displayName":"Susan Malkin","userId":"07747390338629969515"}},"outputId":"2163684a-0fd3-4f0f-cb6c-d10d7d82c93f"},"source":["batch_words, batch_tags = zip(*[zip(*sentence) for sentence in data[-3:]])\n","\n","print(\"Word ids:\")\n","print(to_matrix(batch_words,word_to_id))\n","print(\"Tag ids:\")\n","print(to_matrix(batch_tags,tag_to_id))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Word ids:\n","[[   2 3057    5    2 2238 1334 4238 2454    3    6   19   26 1070   69\n","     8 2088    6    3    1    3  266   65  342    2    1    3    2  315\n","     1    9   87  216 3322   69 1558    4    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0]\n"," [  45   12    8  511 8419    6   60 3246   39    2    1    1    3    2\n","   845    1    3    1    3   10 9910    2    1 3470    9   43    1    1\n","     3    6    2 1046  385   73 4562    3    9    2    1    1 3250    3\n","    12   10    2  861 5240   12    8 8936  121    1    4]\n"," [  33   64   26   12  445    7 7346    9    8 3337    3    1 2811    3\n","     2  463  572    2    1    1 1649   12    1    4    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0]]\n","Tag ids:\n","[[ 6  3  4  6  3  3  9  9  7 12  4  5  9  4  6  3 12  7  9  7  9  8  4  6\n","   3  7  6 13  3  4  6  3  9  4  3  7  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0]\n"," [ 5  9  6  9  3 12  6  3  7  6 13  3  7  6 13  3  7 13  7  5  9  6  3  3\n","   4  6 13  3  7 12  6  3  6 13  3  7  4  6  3  9  3  7  9  4  6 13  3  9\n","   6  3  2 13  7]\n"," [ 4  6  5  9 13  4  3  4  6 13  7 13  3  7  6  3  4  6 13  3  3  9  9  7\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0]]\n"]}]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"rT8HO5rVXJoW"},"source":["### Build and train a simple model\n","\n","In this lab we'll focus on a high-level PyTorch interface to recurrent neural networks, which we tried at the end of the previous lab."]},{"cell_type":"code","metadata":{"id":"wBqW72t7XJoX"},"source":["import torch\n","from torch import nn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S0A3N-4pXJoX"},"source":["class SimpleModel(nn.Module):\n","    \n","    def __init__(self):\n","        super().__init__()\n","        \n","        self.rnn = nn.Sequential(\n","            nn.Embedding(len(all_words), 64),\n","            nn.RNN(64, 64, batch_first=True)\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Linear(64, len(all_tags)),\n","        )\n","    \n","    def forward(self, input):\n","        output, _ = self.rnn(input)\n","        return self.classifier(output)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DKijbze4XJoY"},"source":["We will use data generator for batch traiing:"]},{"cell_type":"code","metadata":{"id":"OmksTBr0XJoY"},"source":["BATCH_SIZE = 128\n","\n","def generate_batches(sentences, batch_size=BATCH_SIZE, max_len=None, pad=0):\n","    assert isinstance(sentences, np.ndarray), \"Make sure sentences is a numpy array\"\n","    \n","    while True:\n","        indices = np.random.permutation(np.arange(len(sentences)))\n","        for start in range(0, len(indices) - 1, batch_size):\n","            batch_indices = indices[start:start + batch_size]\n","            batch_words, batch_tags = [],[]\n","            for sent in sentences[batch_indices]:\n","                words,tags = zip(*sent)\n","                batch_words.append(words)\n","                batch_tags.append(tags)\n","\n","            batch_words = to_matrix(batch_words, word_to_id, max_len, pad)\n","            batch_tags = to_matrix(batch_tags, tag_to_id, max_len, pad)\n","\n","            yield batch_words, batch_tags\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QWOPvdXlcMHc"},"source":["# import stuff\n","from torch import optim\n","\n","from tqdm import tqdm\n","from itertools import islice\n","\n","#auxiliary stuff\n","class AverageMeter:\n","    \n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wOZ9fMfNXJoZ"},"source":["### Training\n","Here we do not compute loss for padded symbols by using argument ignore_index of CrossEntropyLoss."]},{"cell_type":"code","metadata":{"id":"RezsxflyXJob"},"source":["NUM_EPOCH = 10\n","DEVICE = torch.device('cuda')\n","\n","model = SimpleModel().to(DEVICE)\n","optimizer = optim.Adam(model.parameters(), 1e-3)\n","\n","# ignore padding index\n","criterion = nn.CrossEntropyLoss(ignore_index=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dYtTvq7gXJob","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655327909351,"user_tz":240,"elapsed":21908,"user":{"displayName":"Susan Malkin","userId":"07747390338629969515"}},"outputId":"75215d02-e93f-46d4-a5b4-acb4bc50fcfd"},"source":["for _ in range(NUM_EPOCH):\n","\n","    loss_meter = AverageMeter()\n","    for batch in islice(generate_batches(train_data), 0, len(train_data) // BATCH_SIZE):\n","        word_id, tag_id = batch\n","        word_id = torch.from_numpy(word_id).long().to(DEVICE)\n","        tag_id = torch.from_numpy(tag_id).long().to(DEVICE)\n","        \n","        logits = model(word_id).transpose(-1, -2)\n","        \n","        loss = criterion(logits, tag_id)\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        loss_meter.update(loss.item())\n","    \n","    print(loss_meter.avg)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9433784815802503\n","0.3713996912116435\n","0.2485774821309901\n","0.19659109039982753\n","0.17134290141845818\n","0.15717434082458268\n","0.148181233410515\n","0.1418686525590384\n","0.1371195256487647\n","0.13305384988215432\n"]}]},{"cell_type":"code","metadata":{"id":"Obv6ezi6iWNo"},"source":["def compute_test_accuracy(model):\n","    test_words, test_tags = zip(*[zip(*sentence) for sentence in test_data])\n","    test_words, test_tags = to_matrix(test_words, word_to_id), to_matrix(test_tags, tag_to_id)\n","\n","    test_words = torch.from_numpy(test_words).long().to(DEVICE)\n","    test_tags = torch.from_numpy(test_tags).long().to(DEVICE)\n","\n","    predicted_tag_probabilities = model(test_words).argmax(dim=-1)\n","\n","    numerator = torch.sum(torch.logical_and((predicted_tag_probabilities == test_tags), (test_tags != 0)))\n","    denominator = torch.sum(test_words != 0)\n","    accuracy = (numerator / denominator).item()\n","    return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IIYQ5ZRXZB6r"},"source":["accuracy = compute_test_accuracy(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RGjUQ4R0Yyoy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655327922182,"user_tz":240,"elapsed":219,"user":{"displayName":"Susan Malkin","userId":"07747390338629969515"}},"outputId":"ae2c9982-ac55-4a49-ca58-7762a8754ab7"},"source":["print(\"Final accuracy: %.5f\" % accuracy)\n","\n","assert accuracy > 0.94"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Final accuracy: 0.94673\n"]}]},{"cell_type":"markdown","metadata":{"id":"IX3AjVt6XJoe"},"source":["### Task I: getting all bidirectional\n","\n","Since we're analyzing a full sequence, it's legal for us to look into future data.\n","\n","A simple way to achieve that is to go both directions at once, making a __bidirectional RNN__.\n","\n","Try to set argument `bidirectional` to True in `nn.RNN`. You will need to adjust dimensions of rnn layer too!\n","\n","Your first task is to use such a layer for our POS-tagger."]},{"cell_type":"code","metadata":{"id":"A8h3k8VeXJof"},"source":["# Define a model that utilizes bidirectional nn.RNN\n","\n","### Your code here ###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vz1DM5X1XJog"},"source":["acc = compute_test_accuracy(model)\n","print(\"\\nFinal accuracy: %.5f\"%acc)\n","\n","assert acc>0.96, \"Bidirectional RNNs are better than this!\"\n","print(\"Well done!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RdW2nBw-XJog"},"source":["### Task II: now go and improve it\n","\n","You guesses it. We're now gonna ask you to come up with a better network.\n","\n","Here's a few tips:\n","\n","* __Go beyond nn.RNN__: there's `nn.LSTM` and `nn.GRU`\n","  * You can also use 1D Convolutions (`nn.Conv1d`). They are often as good as recurrent layers but with less overfitting.\n","* __Stack more layers__: if there is a common motif to this course it's about stacking layers\n","  * Try to add recurrent and 1dconv layers on top of one another\n","  * Just remember that bigger networks may need more epochs to train\n","* __Gradient clipping__: If your training isn't as stable as you'd like, try to use `nn.utils.clip_grad_norm_`.\n","  * Which is to say, it's a good idea to watch over your loss curve at each minibatch. \n","* __Regularization__: you can apply dropouts as usuall but also in an RNN-specific way\n","  * `nn.Dropout` works inbetween RNN layers\n","  * Recurrent layers also have `dropout` parameter\n","* __More words!__: You can obtain greater performance by expanding your model's input dictionary from 5000 to up to every single word!\n","  * Just make sure your model doesn't overfit due to so many parameters.\n","  * Combined with regularizers or pre-trained word-vectors this could be really good because right now our model is blind to >5% of words.\n","* __The most important advice__: don't cram in everything at once!\n","  * If you stuff in a lot of modiffications, some of them almost inevitably gonna be detrimental and you'll never know which of them are.\n","  * Try to instead go in small iterations and record experiment results to guide further search.\n","  \n","There's some advanced stuff waiting at the end of the notebook.\n","  \n","Good hunting!"]},{"cell_type":"code","metadata":{"id":"Hm_NIQ5rXJoh"},"source":["# <Your code here!>"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6RXKBxmxXJoh"},"source":["acc = compute_test_accuracy(model)\n","print(\"\\nFinal accuracy: %.5f\"%acc)\n","\n","if acc >= 0.99:\n","    print(\"Awesome! Sky was the limit and yet you scored even higher!\")\n","elif acc >= 0.98:\n","    print(\"Excellent! Whatever dark magic you used, it certainly did it's trick.\")\n","elif acc >= 0.97:\n","    print(\"Well done! If this was a graded assignment, you would have gotten a 100% score.\")\n","elif acc > 0.96:\n","    print(\"Just a few more iterations!\")\n","else:\n","    print(\"There seems to be something broken in the model. Unless you know what you're doing, try taking bidirectional RNN and adding one enhancement at a time to see where's the problem.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Isvs4IIXJoi"},"source":["```\n","\n","```\n","\n","```\n","\n","```\n","\n","```\n","\n","```\n","\n","\n","#### Some advanced stuff\n","Here there are a few more tips on how to improve training that are a bit trickier to impliment. We strongly suggest that you try them _after_ you've got a good initial model.\n","* __Use pre-trained embeddings__: you can use pre-trained weights from [there](http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/) to kickstart your Embedding layer.\n","  * Use nn.Embedding.from_pretrained to init the embedding layer with a pretrained matrix.\n","  * When using pre-trained embeddings, pay attention to the fact that model's dictionary is different from your own.\n","  * You may want to freeze the parameters of embedding layer for several first epoches of fine-tuning or to not fine-tune them at all. In the first case you can choose zero learning rate for this parameter group, in the second case just use the freeze argument of nn.Embedding.from_pretrained.\n","* __More efficient batching__: right now it spends a lot of time iterating over \"0\"s\n","  * This happens because batch is always padded to the length of a longest sentence\n","  * You can speed things up by pre-generating batches of similar lengths and feeding it with randomly chosen pre-generated batch.\n","  * This technically breaks the i.i.d. assumption, but it works unless you come up with some insane rnn architectures.\n","* __Structured loss functions__: since we're tagging the whole sequence at once, we might as well train our network to do so.\n","  * There's more than one way to do so, but we'd recommend starting with [Conditional Random Fields](http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/)\n","  * You can read an [official PyTorch tutorial on Bi-LSTM Conditional Random Field](https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html#bi-lstm-conditional-random-field-discussion).\n"]}]}