{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"10P3EISgqPQUxqSTEA6bA-3jQggdl2k4w","timestamp":1664052443884},{"file_id":"1ym8XfMZr03cdSJu8JBuQ0whLergZ_HrW","timestamp":1664033617583},{"file_id":"1ltJmYjAVXFRWeVYIcfkKL6nFu1VPUxjK","timestamp":1663704869182}],"collapsed_sections":[],"private_outputs":true,"machine_shape":"hm","background_execution":"on"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"855F0BdB_oVb"},"source":["# Lane Detection Project\n","\n","In this project you need to construct computer vision solution for a lane detection. We are going to implement [LaneNet](https://arxiv.org/pdf/1802.05591.pdf) paper. Our solution is going to have the following parts:\n","\n","1. Lane semantic segmentation and metric learning\n","2. Instance segmentation over embedings\n","3. Homography prediction\n","4. Lane fitting\n","\n","We are going to use [TuSimple](https://github.com/TuSimple/tusimple-benchmark/issues/3) dataset for a lane detection.\n","\n","Let's discuss every step in details.\n","\n","## Semantic and Instance Segmentation\n","\n","In this part you should implement and train neural network with the 2 heads. The model should take as an input image and returns to masks. One head should predict *semantic segmentation* and another head should predict *embedding for each pixel*. Pixels that are part of one lane should have similar embeddings, on the other hand pixels from different lanes should have high distance between embeddings. In order to do that, we should implement *discriminative loss*.\n","\n","After training the discribed network, we can threshold pixels of the interest and cluster them according to the embedding with the *DBSCAN*.\n","\n","After solving the problem you should obtain the following result. \n","\n","## Homograpy Prediction\n","\n","In order to solve the problem, we would like to understand an exact lane geometry. We are going to describe the lane with the 2-nd order polynom $ax^2 + bx + c$. \n","\n","In the image plane lines are not parallel and we may need higher order polynoms to discribe the lane geometry. In order to avoid that we could project image in the bird's eye view with the homography. But drivable surface has different properties over frames, so we would like to condition the homography on the image.\n","\n","In order to do that, we going to train separate network to predict the best homography for each image.\n","\n","## Lane Fitting\n","\n","After predicting homography and lane's clustering we will fit 2-nd order polynom for each lane to obtain the final result. Lane fitting can be formulated as a least-squares problem, where we have points $\\{x_i, y_i\\}_{i=1}^n$ and we would like to estimation coefficients $a, b, c$ s.t.\n","\n","$$\n","\\sum_{i = 1}^n ||y_i - a x_i^2 - bx_i - c||^2 \\rightarrow \\min_{a, b,c}\n","$$\n","\n","\n","\n","## Important Notes\n","0. During the project we provide the relevant papers. Reading the papers can be very helpful (and sometimes necessary) to implement the code.\n","1. During the project we provide you with the code templates that you should fill.\n","2. Homography Prediction and Instance Segmentation can be done in parallel.\n","3. You are free to use any other environment instead of Google Colab.\n","4. You can deviate from the original papers if it helps you to solve the problem, but you should explain your solution and motivation in the text block."]},{"cell_type":"markdown","metadata":{"id":"8H2xVCZoILE9"},"source":["## Semantic and Segmentation Code"]},{"cell_type":"markdown","metadata":{"id":"1oVf4lYMMuXN"},"source":["You can use the following commands to download the dataset and unpack it. But we suggest to upload the data to the Google Drive, with Google Drive you will be able to access the data much faster."]},{"cell_type":"code","metadata":{"id":"ufhwRLIK_nwu"},"source":["#!mkdir -p /data/tusimple\n","#!wget https://s3.us-east-2.amazonaws.com/benchmark-frontend/datasets/1/train_set.zip\n","#!unzip train_set.zip -d /data/tusimple  #"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"SM9VlHbb12V3"}},{"cell_type":"code","source":["!python --version"],"metadata":{"id":"FrjMDSQ0NU28"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --upgrade torchvision==0.12"],"metadata":{"id":"-hToRieNT4Zl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","from google.colab.patches import cv2_imshow\n","\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms.functional as TF\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.transforms import ToTensor\n","from torchvision import datasets, transforms\n","\n","import os\n","from PIL import Image\n","import random\n","from skimage.transform import resize\n","\n","from torch.nn import Module, Sequential, Conv2d, ReLU, AdaptiveMaxPool2d, AdaptiveAvgPool2d, \\\n","    NLLLoss, BCELoss, CrossEntropyLoss, AvgPool2d, MaxPool2d, Parameter, Linear, Sigmoid, Softmax, Dropout, Embedding\n","from torch.nn import functional as F\n","from torch.autograd import Variable\n","\n","from collections import OrderedDict\n","\n","import torchvision.models as models"],"metadata":{"id":"J1VvLa6GtCnK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"-GCp4Of34WBa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!unzip /content/drive/MyDrive/train_set.zip -d /content/drive/MyDrive"],"metadata":{"id":"GTWkZpcI4z0D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!python /content/drive/MyDrive/tusimple_transform.py --src_dir /content/drive/MyDrive"],"metadata":{"id":"L1xglE8ZSl8q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J_VWzDc7NKsi"},"source":["In the following cell you should implement pytorch Dataset to generate training examples for instance and semantic segmentation. You are able to modify the signatures if you want.\n","\n","To implement the LaneDataset you should become familiar with the labeling format.\n","\n","The dataset is relatively small, so it is important to implement different augmentation strategies. We suggest to use random flips and color augmentations (brightness, saturation). You can also use augmentations of your choice. Augmentations are not required to obtain a full grade but without them your model could overfit dramatically.\n","\n","NOTE: It can be useful to generate binary segmentation and instance segmentation ground truth once and then use it as-is in every experiment."]},{"cell_type":"code","source":["class LaneDataSet(Dataset):\n","    def __init__(self, dataset, n_labels=5, transform=None):\n","        self._gt_img_list = []\n","        self._gt_label_binary_list = []\n","        self._gt_label_instance_list = []\n","        self.transform = transform\n","        self.n_labels = n_labels\n","\n","        with open(dataset, 'r') as file:\n","            for _info in file:\n","                info_tmp = _info.strip(' ').split()\n","\n","                self._gt_img_list.append(info_tmp[0])\n","                self._gt_label_binary_list.append(info_tmp[1])\n","                self._gt_label_instance_list.append(info_tmp[2])\n","\n","        assert len(self._gt_img_list) == len(self._gt_label_binary_list) == len(self._gt_label_instance_list)\n","\n","        self._shuffle()\n","\n","    def _shuffle(self):\n","        # randomly shuffle all list identically\n","        c = list(zip(self._gt_img_list, self._gt_label_binary_list, self._gt_label_instance_list))\n","        random.shuffle(c)\n","        self._gt_img_list, self._gt_label_binary_list, self._gt_label_instance_list = zip(*c)\n","\n","    def _split_instance_gt(self, label_instance_img):\n","        # number of channels, number of unique pixel values, subtracting no label\n","        # adapted from here https://github.com/nyoki-mtl/pytorch-discriminative-loss/blob/master/src/dataset.py\n","        no_of_instances = self.n_labels\n","        ins = np.zeros((no_of_instances, label_instance_img.shape[0], label_instance_img.shape[1]))\n","        for _ch, label in enumerate(np.unique(label_instance_img)[1:]):\n","            ins[_ch, label_instance_img == label] = 1\n","\n","        return ins\n","\n","    def __len__(self):\n","        return len(self._gt_img_list)\n","\n","    def __getitem__(self, idx):\n","        assert len(self._gt_label_binary_list) == len(self._gt_label_instance_list) \\\n","               == len(self._gt_img_list)\n","\n","        # load all\n","        img = cv2.imread(self._gt_img_list[idx], cv2.IMREAD_COLOR)\n","\n","        label_instance_img = cv2.imread(self._gt_label_instance_list[idx], cv2.IMREAD_UNCHANGED)\n","\n","        label_img = cv2.imread(self._gt_label_binary_list[idx], cv2.IMREAD_COLOR)\n","\n","        # optional transformations\n","        if self.transform:\n","            img = self.transform(img)\n","            label_img = self.transform(label_img)\n","            label_instance_img = self.transform(label_instance_img)\n","\n","        # extract each label into separate binary channels\n","        label_instance_img = self._split_instance_gt(label_instance_img)\n","\n","        # reshape for pytorch\n","        # tensorflow: [height, width, channels]\n","        # pytorch: [channels, height, width]\n","        img = img.reshape(img.shape[2], img.shape[0], img.shape[1])\n","\n","        label_binary = np.zeros([label_img.shape[0], label_img.shape[1]], dtype=np.uint8)\n","        mask = np.where((label_img[:, :, :] != [0, 0, 0]).all(axis=2))\n","        label_binary[mask] = 1\n","\n","        # we could split the instance label here, each instance in one channel (basically a binary mask for each)\n","        return img, label_binary, label_instance_img"],"metadata":{"id":"T0Y_bUjGfSrt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ds = LaneDataSet(os.path.join('/content/drive/MyDrive/training/', 'train.txt'))"],"metadata":{"id":"PuaShoDerR2t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ds1 = ds[0]\n","#ds1[2].shape"],"metadata":{"id":"gzLwSQ8pr-EI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Rescale():\n","    \"\"\"Rescale the image in a sample to a given size.\n","    Args:\n","        output_size (width, height) (tuple): Desired output size (width, height). Output is\n","            matched to output_size.\n","    \"\"\"\n","\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, (tuple))\n","        self.output_size = output_size\n","\n","    def __call__(self, sample):\n","        sample = cv2.resize(sample, dsize=self.output_size, interpolation=cv2.INTER_NEAREST)\n","\n","        return sample"],"metadata":{"id":"7RFVm6Y8gMIl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class InitialBlock(nn.Module):\n","    \"\"\"The initial block is composed of two branches:\n","    1. a main branch which performs a regular convolution with stride 2;\n","    2. an extension branch which performs max-pooling.\n","    Doing both operations in parallel and concatenating their results\n","    allows for efficient downsampling and expansion. The main branch\n","    outputs 13 feature maps while the extension branch outputs 3, for a\n","    total of 16 feature maps after concatenation.\n","    Keyword arguments:\n","    - in_channels (int): the number of input channels.\n","    - out_channels (int): the number output channels.\n","    - kernel_size (int, optional): the kernel size of the filters used in\n","    the convolution layer. Default: 3.\n","    - padding (int, optional): zero-padding added to both sides of the\n","    input. Default: 0.\n","    - bias (bool, optional): Adds a learnable bias to the output if\n","    ``True``. Default: False.\n","    - relu (bool, optional): When ``True`` ReLU is used as the activation\n","    function; otherwise, PReLU is used. Default: True.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 kernel_size=3,\n","                 padding=0,\n","                 bias=False,\n","                 relu=True):\n","        super().__init__()\n","\n","        if relu:\n","            activation = nn.ReLU()\n","        else:\n","            activation = nn.PReLU()\n","\n","        # Main branch - As stated above the number of output channels for this\n","        # branch is the total minus 3, since the remaining channels come from\n","        # the extension branch\n","        self.main_branch = nn.Conv2d(\n","            in_channels,\n","            out_channels - 3,\n","            kernel_size=kernel_size,\n","            stride=2,\n","            padding=padding,\n","            bias=bias)\n","\n","        # Extension branch\n","        self.ext_branch = nn.MaxPool2d(kernel_size, stride=2, padding=padding)\n","\n","        # Initialize batch normalization to be used after concatenation\n","        self.batch_norm = nn.BatchNorm2d(out_channels)\n","\n","        # PReLU layer to apply after concatenating the branches\n","        self.out_prelu = activation\n","\n","    def forward(self, x):\n","        main = self.main_branch(x)\n","        ext = self.ext_branch(x)\n","\n","        # Concatenate branches\n","        out = torch.cat((main, ext), 1)\n","\n","        # Apply batch normalization\n","        out = self.batch_norm(out)\n","\n","        return self.out_prelu(out)\n","\n","\n","class RegularBottleneck(nn.Module):\n","    \"\"\"Regular bottlenecks are the main building block of ENet.\n","    Main branch:\n","    1. Shortcut connection.\n","    Extension branch:\n","    1. 1x1 convolution which decreases the number of channels by\n","    ``internal_ratio``, also called a projection;\n","    2. regular, dilated or asymmetric convolution;\n","    3. 1x1 convolution which increases the number of channels back to\n","    ``channels``, also called an expansion;\n","    4. dropout as a regularizer.\n","    Keyword arguments:\n","    - channels (int): the number of input and output channels.\n","    - internal_ratio (int, optional): a scale factor applied to\n","    ``channels`` used to compute the number of\n","    channels after the projection. eg. given ``channels`` equal to 128 and\n","    internal_ratio equal to 2 the number of channels after the projection\n","    is 64. Default: 4.\n","    - kernel_size (int, optional): the kernel size of the filters used in\n","    the convolution layer described above in item 2 of the extension\n","    branch. Default: 3.\n","    - padding (int, optional): zero-padding added to both sides of the\n","    input. Default: 0.\n","    - dilation (int, optional): spacing between kernel elements for the\n","    convolution described in item 2 of the extension branch. Default: 1.\n","    asymmetric (bool, optional): flags if the convolution described in\n","    item 2 of the extension branch is asymmetric or not. Default: False.\n","    - dropout_prob (float, optional): probability of an element to be\n","    zeroed. Default: 0 (no dropout).\n","    - bias (bool, optional): Adds a learnable bias to the output if\n","    ``True``. Default: False.\n","    - relu (bool, optional): When ``True`` ReLU is used as the activation\n","    function; otherwise, PReLU is used. Default: True.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 channels,\n","                 internal_ratio=4,\n","                 kernel_size=3,\n","                 padding=0,\n","                 dilation=1,\n","                 asymmetric=False,\n","                 dropout_prob=0,\n","                 bias=False,\n","                 relu=True):\n","        super().__init__()\n","\n","        # Check in the internal_scale parameter is within the expected range\n","        # [1, channels]\n","        if internal_ratio <= 1 or internal_ratio > channels:\n","            raise RuntimeError(\"Value out of range. Expected value in the \"\n","                               \"interval [1, {0}], got internal_scale={1}.\"\n","                               .format(channels, internal_ratio))\n","\n","        internal_channels = channels // internal_ratio\n","\n","        if relu:\n","            activation = nn.ReLU()\n","        else:\n","            activation = nn.PReLU()\n","\n","        # Main branch - shortcut connection\n","\n","        # Extension branch - 1x1 convolution, followed by a regular, dilated or\n","        # asymmetric convolution, followed by another 1x1 convolution, and,\n","        # finally, a regularizer (spatial dropout). Number of channels is constant.\n","\n","        # 1x1 projection convolution\n","        self.ext_conv1 = nn.Sequential(\n","            nn.Conv2d(\n","                channels,\n","                internal_channels,\n","                kernel_size=1,\n","                stride=1,\n","                bias=bias), nn.BatchNorm2d(internal_channels), activation)\n","\n","        # If the convolution is asymmetric we split the main convolution in\n","        # two. Eg. for a 5x5 asymmetric convolution we have two convolution:\n","        # the first is 5x1 and the second is 1x5.\n","        if asymmetric:\n","            self.ext_conv2 = nn.Sequential(\n","                nn.Conv2d(\n","                    internal_channels,\n","                    internal_channels,\n","                    kernel_size=(kernel_size, 1),\n","                    stride=1,\n","                    padding=(padding, 0),\n","                    dilation=dilation,\n","                    bias=bias), nn.BatchNorm2d(internal_channels), activation,\n","                nn.Conv2d(\n","                    internal_channels,\n","                    internal_channels,\n","                    kernel_size=(1, kernel_size),\n","                    stride=1,\n","                    padding=(0, padding),\n","                    dilation=dilation,\n","                    bias=bias), nn.BatchNorm2d(internal_channels), activation)\n","        else:\n","            self.ext_conv2 = nn.Sequential(\n","                nn.Conv2d(\n","                    internal_channels,\n","                    internal_channels,\n","                    kernel_size=kernel_size,\n","                    stride=1,\n","                    padding=padding,\n","                    dilation=dilation,\n","                    bias=bias), nn.BatchNorm2d(internal_channels), activation)\n","\n","        # 1x1 expansion convolution\n","        self.ext_conv3 = nn.Sequential(\n","            nn.Conv2d(\n","                internal_channels,\n","                channels,\n","                kernel_size=1,\n","                stride=1,\n","                bias=bias), nn.BatchNorm2d(channels), activation)\n","\n","        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n","\n","        # PReLU layer to apply after adding the branches\n","        self.out_prelu = activation\n","\n","    def forward(self, x):\n","        # Main branch shortcut\n","        main = x\n","\n","        # Extension branch\n","        ext = self.ext_conv1(x)\n","        ext = self.ext_conv2(ext)\n","        ext = self.ext_conv3(ext)\n","        ext = self.ext_regul(ext)\n","\n","        # Add main and extension branches\n","        out = main + ext\n","\n","        return self.out_prelu(out)\n","\n","\n","class DownsamplingBottleneck(nn.Module):\n","    \"\"\"Downsampling bottlenecks further downsample the feature map size.\n","    Main branch:\n","    1. max pooling with stride 2; indices are saved to be used for\n","    unpooling later.\n","    Extension branch:\n","    1. 2x2 convolution with stride 2 that decreases the number of channels\n","    by ``internal_ratio``, also called a projection;\n","    2. regular convolution (by default, 3x3);\n","    3. 1x1 convolution which increases the number of channels to\n","    ``out_channels``, also called an expansion;\n","    4. dropout as a regularizer.\n","    Keyword arguments:\n","    - in_channels (int): the number of input channels.\n","    - out_channels (int): the number of output channels.\n","    - internal_ratio (int, optional): a scale factor applied to ``channels``\n","    used to compute the number of channels after the projection. eg. given\n","    ``channels`` equal to 128 and internal_ratio equal to 2 the number of\n","    channels after the projection is 64. Default: 4.\n","    - kernel_size (int, optional): the kernel size of the filters used in\n","    the convolution layer described above in item 2 of the extension branch.\n","    Default: 3.\n","    - padding (int, optional): zero-padding added to both sides of the\n","    input. Default: 0.\n","    - dilation (int, optional): spacing between kernel elements for the\n","    convolution described in item 2 of the extension branch. Default: 1.\n","    - asymmetric (bool, optional): flags if the convolution described in\n","    item 2 of the extension branch is asymmetric or not. Default: False.\n","    - return_indices (bool, optional):  if ``True``, will return the max\n","    indices along with the outputs. Useful when unpooling later.\n","    - dropout_prob (float, optional): probability of an element to be\n","    zeroed. Default: 0 (no dropout).\n","    - bias (bool, optional): Adds a learnable bias to the output if\n","    ``True``. Default: False.\n","    - relu (bool, optional): When ``True`` ReLU is used as the activation\n","    function; otherwise, PReLU is used. Default: True.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 internal_ratio=4,\n","                 kernel_size=3,\n","                 padding=0,\n","                 return_indices=False,\n","                 dropout_prob=0,\n","                 bias=False,\n","                 relu=True):\n","        super().__init__()\n","\n","        # Store parameters that are needed later\n","        self.return_indices = return_indices\n","\n","        # Check in the internal_scale parameter is within the expected range\n","        # [1, channels]\n","        if internal_ratio <= 1 or internal_ratio > in_channels:\n","            raise RuntimeError(\"Value out of range. Expected value in the \"\n","                               \"interval [1, {0}], got internal_scale={1}. \"\n","                               .format(in_channels, internal_ratio))\n","\n","        internal_channels = in_channels // internal_ratio\n","\n","        if relu:\n","            activation = nn.ReLU()\n","        else:\n","            activation = nn.PReLU()\n","\n","        # Main branch - max pooling followed by feature map (channels) padding\n","        self.main_max1 = nn.MaxPool2d(\n","            kernel_size,\n","            stride=2,\n","            padding=padding,\n","            return_indices=return_indices)\n","\n","        # Extension branch - 2x2 convolution, followed by a regular, dilated or\n","        # asymmetric convolution, followed by another 1x1 convolution. Number\n","        # of channels is doubled.\n","\n","        # 2x2 projection convolution with stride 2\n","        self.ext_conv1 = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels,\n","                internal_channels,\n","                kernel_size=2,\n","                stride=2,\n","                bias=bias), nn.BatchNorm2d(internal_channels), activation)\n","\n","        # Convolution\n","        self.ext_conv2 = nn.Sequential(\n","            nn.Conv2d(\n","                internal_channels,\n","                internal_channels,\n","                kernel_size=kernel_size,\n","                stride=1,\n","                padding=padding,\n","                bias=bias), nn.BatchNorm2d(internal_channels), activation)\n","\n","        # 1x1 expansion convolution\n","        self.ext_conv3 = nn.Sequential(\n","            nn.Conv2d(\n","                internal_channels,\n","                out_channels,\n","                kernel_size=1,\n","                stride=1,\n","                bias=bias), nn.BatchNorm2d(out_channels), activation)\n","\n","        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n","\n","        # PReLU layer to apply after concatenating the branches\n","        self.out_prelu = activation\n","\n","    def forward(self, x):\n","        # Main branch shortcut\n","        if self.return_indices:\n","            main, max_indices = self.main_max1(x)\n","        else:\n","            main = self.main_max1(x)\n","\n","        # Extension branch\n","        ext = self.ext_conv1(x)\n","        ext = self.ext_conv2(ext)\n","        ext = self.ext_conv3(ext)\n","        ext = self.ext_regul(ext)\n","\n","        # Main branch channel padding\n","        n, ch_ext, h, w = ext.size()\n","        ch_main = main.size()[1]\n","        padding = torch.zeros(n, ch_ext - ch_main, h, w)\n","\n","        # Before concatenating, check if main is on the CPU or GPU and\n","        # convert padding accordingly\n","        if main.is_cuda:\n","            padding = padding.cuda()\n","\n","        # Concatenate\n","        main = torch.cat((main, padding), 1)\n","\n","        # Add main and extension branches\n","        out = main + ext\n","\n","        return self.out_prelu(out), max_indices\n","\n","\n","class UpsamplingBottleneck(nn.Module):\n","    \"\"\"The upsampling bottlenecks upsample the feature map resolution using max\n","    pooling indices stored from the corresponding downsampling bottleneck.\n","    Main branch:\n","    1. 1x1 convolution with stride 1 that decreases the number of channels by\n","    ``internal_ratio``, also called a projection;\n","    2. max unpool layer using the max pool indices from the corresponding\n","    downsampling max pool layer.\n","    Extension branch:\n","    1. 1x1 convolution with stride 1 that decreases the number of channels by\n","    ``internal_ratio``, also called a projection;\n","    2. transposed convolution (by default, 3x3);\n","    3. 1x1 convolution which increases the number of channels to\n","    ``out_channels``, also called an expansion;\n","    4. dropout as a regularizer.\n","    Keyword arguments:\n","    - in_channels (int): the number of input channels.\n","    - out_channels (int): the number of output channels.\n","    - internal_ratio (int, optional): a scale factor applied to ``in_channels``\n","     used to compute the number of channels after the projection. eg. given\n","     ``in_channels`` equal to 128 and ``internal_ratio`` equal to 2 the number\n","     of channels after the projection is 64. Default: 4.\n","    - kernel_size (int, optional): the kernel size of the filters used in the\n","    convolution layer described above in item 2 of the extension branch.\n","    Default: 3.\n","    - padding (int, optional): zero-padding added to both sides of the input.\n","    Default: 0.\n","    - dropout_prob (float, optional): probability of an element to be zeroed.\n","    Default: 0 (no dropout).\n","    - bias (bool, optional): Adds a learnable bias to the output if ``True``.\n","    Default: False.\n","    - relu (bool, optional): When ``True`` ReLU is used as the activation\n","    function; otherwise, PReLU is used. Default: True.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 internal_ratio=4,\n","                 kernel_size=3,\n","                 padding=0,\n","                 dropout_prob=0,\n","                 bias=False,\n","                 relu=True):\n","        super().__init__()\n","\n","        # Check in the internal_scale parameter is within the expected range\n","        # [1, channels]\n","        if internal_ratio <= 1 or internal_ratio > in_channels:\n","            raise RuntimeError(\"Value out of range. Expected value in the \"\n","                               \"interval [1, {0}], got internal_scale={1}. \"\n","                               .format(in_channels, internal_ratio))\n","\n","        internal_channels = in_channels // internal_ratio\n","\n","        if relu:\n","            activation = nn.ReLU()\n","        else:\n","            activation = nn.PReLU()\n","\n","        # Main branch - max pooling followed by feature map (channels) padding\n","        self.main_conv1 = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias),\n","            nn.BatchNorm2d(out_channels))\n","\n","        # Remember that the stride is the same as the kernel_size, just like\n","        # the max pooling layers\n","        self.main_unpool1 = nn.MaxUnpool2d(kernel_size=2)\n","\n","        # Extension branch - 1x1 convolution, followed by a regular, dilated or\n","        # asymmetric convolution, followed by another 1x1 convolution. Number\n","        # of channels is doubled.\n","\n","        # 1x1 projection convolution with stride 1\n","        self.ext_conv1 = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels, internal_channels, kernel_size=1, bias=bias),\n","            nn.BatchNorm2d(internal_channels), activation)\n","\n","        # Transposed convolution\n","        self.ext_conv2 = nn.Sequential(\n","            nn.ConvTranspose2d(\n","                internal_channels,\n","                internal_channels,\n","                kernel_size=kernel_size,\n","                stride=2,\n","                padding=padding,\n","                output_padding=1,\n","                bias=bias), nn.BatchNorm2d(internal_channels), activation)\n","\n","        # 1x1 expansion convolution\n","        self.ext_conv3 = nn.Sequential(\n","            nn.Conv2d(\n","                internal_channels, out_channels, kernel_size=1, bias=bias),\n","            nn.BatchNorm2d(out_channels), activation)\n","\n","        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n","\n","        # PReLU layer to apply after concatenating the branches\n","        self.out_prelu = activation\n","\n","    def forward(self, x, max_indices):\n","        # Main branch shortcut\n","        main = self.main_conv1(x)\n","        main = self.main_unpool1(main, max_indices)\n","        # Extension branch\n","        ext = self.ext_conv1(x)\n","        ext = self.ext_conv2(ext)\n","        ext = self.ext_conv3(ext)\n","        ext = self.ext_regul(ext)\n","\n","        # Add main and extension branches\n","        out = main + ext\n","\n","        return self.out_prelu(out)\n","\n","\n","class CBR(nn.Module):\n","    '''\n","    This class defines the convolution layer with batch normalization and PReLU activation\n","    '''\n","\n","    def __init__(self, nIn, nOut, kSize, stride=1):\n","        '''\n","        :param nIn: number of input channels\n","        :param nOut: number of output channels\n","        :param kSize: kernel size\n","        :param stride: stride rate for down-sampling. Default is 1\n","        '''\n","        super().__init__()\n","        padding = int((kSize - 1) / 2)\n","        # self.conv = nn.Conv2d(nIn, nOut, kSize, stride=stride, padding=padding, bias=False)\n","        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n","        # self.conv1 = nn.Conv2d(nOut, nOut, (1, kSize), stride=1, padding=(0, padding), bias=False)\n","        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n","        self.act = nn.PReLU(nOut)\n","\n","    def forward(self, input):\n","        '''\n","        :param input: input feature map\n","        :return: transformed feature map\n","        '''\n","        output = self.conv(input)\n","        # output = self.conv1(output)\n","        output = self.bn(output)\n","        output = self.act(output)\n","        return output\n","\n","\n","class BR(nn.Module):\n","    '''\n","        This class groups the batch normalization and PReLU activation\n","    '''\n","\n","    def __init__(self, nOut):\n","        '''\n","        :param nOut: output feature maps\n","        '''\n","        super().__init__()\n","        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n","        self.act = nn.PReLU(nOut)\n","\n","    def forward(self, input):\n","        '''\n","        :param input: input feature map\n","        :return: normalized and thresholded feature map\n","        '''\n","        output = self.bn(input)\n","        output = self.act(output)\n","        return output\n","\n","\n","class CB(nn.Module):\n","    '''\n","       This class groups the convolution and batch normalization\n","    '''\n","\n","    def __init__(self, nIn, nOut, kSize, stride=1):\n","        '''\n","        :param nIn: number of input channels\n","        :param nOut: number of output channels\n","        :param kSize: kernel size\n","        :param stride: optinal stide for down-sampling\n","        '''\n","        super().__init__()\n","        padding = int((kSize - 1) / 2)\n","        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n","        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n","\n","    def forward(self, input):\n","        '''\n","        :param input: input feature map\n","        :return: transformed feature map\n","        '''\n","        output = self.conv(input)\n","        output = self.bn(output)\n","        return output\n","\n","\n","class C(nn.Module):\n","    '''\n","    This class is for a convolutional layer.\n","    '''\n","\n","    def __init__(self, nIn, nOut, kSize, stride=1):\n","        '''\n","        :param nIn: number of input channels\n","        :param nOut: number of output channels\n","        :param kSize: kernel size\n","        :param stride: optional stride rate for down-sampling\n","        '''\n","        super().__init__()\n","        padding = int((kSize - 1) / 2)\n","        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n","\n","    def forward(self, input):\n","        '''\n","        :param input: input feature map\n","        :return: transformed feature map\n","        '''\n","        output = self.conv(input)\n","        return output\n","\n","\n","class CDilated(nn.Module):\n","    '''\n","    This class defines the dilated convolution.\n","    '''\n","\n","    def __init__(self, nIn, nOut, kSize, stride=1, d=1):\n","        '''\n","        :param nIn: number of input channels\n","        :param nOut: number of output channels\n","        :param kSize: kernel size\n","        :param stride: optional stride rate for down-sampling\n","        :param d: optional dilation rate\n","        '''\n","        super().__init__()\n","        padding = int((kSize - 1) / 2) * d\n","        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False,\n","                              dilation=d)\n","\n","    def forward(self, input):\n","        '''\n","        :param input: input feature map\n","        :return: transformed feature map\n","        '''\n","        output = self.conv(input)\n","        return output\n","\n","\n","class DownSamplerB(nn.Module):\n","    def __init__(self, nIn, nOut):\n","        super().__init__()\n","        n = int(nOut / 5)\n","        n1 = nOut - 4 * n\n","        self.c1 = C(nIn, n, 3, 2)\n","        self.d1 = CDilated(n, n1, 3, 1, 1)\n","        self.d2 = CDilated(n, n, 3, 1, 2)\n","        self.d4 = CDilated(n, n, 3, 1, 4)\n","        self.d8 = CDilated(n, n, 3, 1, 8)\n","        self.d16 = CDilated(n, n, 3, 1, 16)\n","        self.bn = nn.BatchNorm2d(nOut, eps=1e-3)\n","        self.act = nn.PReLU(nOut)\n","\n","    def forward(self, input):\n","        output1 = self.c1(input)\n","        d1 = self.d1(output1)\n","        d2 = self.d2(output1)\n","        d4 = self.d4(output1)\n","        d8 = self.d8(output1)\n","        d16 = self.d16(output1)\n","\n","        add1 = d2\n","        add2 = add1 + d4\n","        add3 = add2 + d8\n","        add4 = add3 + d16\n","\n","        combine = torch.cat([d1, add1, add2, add3, add4], 1)\n","        # combine_in_out = input + combine\n","        output = self.bn(combine)\n","        output = self.act(output)\n","        return output\n","\n","\n","class DilatedParallelResidualBlockB(nn.Module):\n","    '''\n","    This class defines the ESP block, which is based on the following principle\n","        Reduce ---> Split ---> Transform --> Merge\n","    '''\n","\n","    def __init__(self, nIn, nOut, add=True):\n","        '''\n","        :param nIn: number of input channels\n","        :param nOut: number of output channels\n","        :param add: if true, add a residual connection through identity operation. You can use projection too as\n","                in ResNet paper, but we avoid to use it if the dimensions are not the same because we do not want to\n","                increase the module complexity\n","        '''\n","        super().__init__()\n","        n = int(nOut / 5)\n","        n1 = nOut - 4 * n\n","        self.c1 = C(nIn, n, 1, 1)\n","        self.d1 = CDilated(n, n1, 3, 1, 1)  # dilation rate of 2^0\n","        self.d2 = CDilated(n, n, 3, 1, 2)  # dilation rate of 2^1\n","        self.d4 = CDilated(n, n, 3, 1, 4)  # dilation rate of 2^2\n","        self.d8 = CDilated(n, n, 3, 1, 8)  # dilation rate of 2^3\n","        self.d16 = CDilated(n, n, 3, 1, 16)  # dilation rate of 2^4\n","        self.bn = BR(nOut)\n","        self.add = add\n","\n","    def forward(self, input):\n","        '''\n","        :param input: input feature map\n","        :return: transformed feature map\n","        '''\n","        # reduce\n","        output1 = self.c1(input)\n","        # split and transform\n","        d1 = self.d1(output1)\n","        d2 = self.d2(output1)\n","        d4 = self.d4(output1)\n","        d8 = self.d8(output1)\n","        d16 = self.d16(output1)\n","\n","        # heirarchical fusion for de-gridding\n","        add1 = d2\n","        add2 = add1 + d4\n","        add3 = add2 + d8\n","        add4 = add3 + d16\n","\n","        # merge\n","        combine = torch.cat([d1, add1, add2, add3, add4], 1)\n","\n","        # if residual version\n","        if self.add:\n","            combine = input + combine\n","        output = self.bn(combine)\n","        return output\n","\n","\n","class InputProjectionA(nn.Module):\n","    '''\n","    This class projects the input image to the same spatial dimensions as the feature map.\n","    For example, if the input image is 512 x512 x3 and spatial dimensions of feature map size are 56x56xF, then\n","    this class will generate an output of 56x56x3\n","    '''\n","\n","    def __init__(self, samplingTimes):\n","        '''\n","        :param samplingTimes: The rate at which you want to down-sample the image\n","        '''\n","        super().__init__()\n","        self.pool = nn.ModuleList()\n","        for i in range(0, samplingTimes):\n","            # pyramid-based approach for down-sampling\n","            self.pool.append(nn.AvgPool2d(3, stride=2, padding=1))\n","\n","    def forward(self, input):\n","        '''\n","        :param input: Input RGB Image\n","        :return: down-sampled image (pyramid-based approach)\n","        '''\n","        for pool in self.pool:\n","            input = pool(input)\n","        return input\n","\n","\n","class DANetHead(nn.Module):\n","    def __init__(self, in_channels, out_channels, norm_layer):\n","        super(DANetHead, self).__init__()\n","        inter_channels = in_channels // 4\n","        self.conv5a = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n","                                    norm_layer(inter_channels),\n","                                    nn.ReLU())\n","\n","        self.conv5c = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n","                                    norm_layer(inter_channels),\n","                                    nn.ReLU())\n","\n","        self.sa = PAM_Module(inter_channels)\n","        self.sc = CAM_Module(inter_channels)\n","        self.conv51 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\n","                                    norm_layer(inter_channels),\n","                                    nn.ReLU())\n","        self.conv52 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\n","                                    norm_layer(inter_channels),\n","                                    nn.ReLU())\n","\n","        self.conv6 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(inter_channels, out_channels, 1, bias=False))\n","        self.conv7 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(inter_channels, out_channels, 1, bias=False))\n","\n","        self.conv8 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(inter_channels, out_channels, 1, bias=False))\n","\n","    def forward(self, x):\n","        feat1 = self.conv5a(x)\n","        sa_feat = self.sa(feat1)\n","        sa_conv = self.conv51(sa_feat)\n","        sa_output = self.conv6(sa_conv)\n","\n","        feat2 = self.conv5c(x)\n","        sc_feat = self.sc(feat2)\n","        sc_conv = self.conv52(sc_feat)\n","        sc_output = self.conv7(sc_conv)\n","\n","        feat_sum = sa_conv + sc_conv\n","\n","        sasc_output = self.conv8(feat_sum)\n","\n","        return sasc_output\n","\n","\n","class PAM_Module(Module):\n","    \"\"\" Position attention module\"\"\"\n","\n","    # Ref from SAGAN\n","    def __init__(self, in_dim):\n","        super(PAM_Module, self).__init__()\n","        self.chanel_in = in_dim\n","\n","        self.query_conv = Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n","        self.key_conv = Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n","        self.value_conv = Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n","        self.gamma = Parameter(torch.zeros(1))\n","\n","        self.softmax = Softmax(dim=-1)\n","\n","    def forward(self, x):\n","        \"\"\"\n","            inputs :\n","                x : input feature maps( B X C X H X W)\n","            returns :\n","                out : attention value + input feature\n","                attention: B X (HxW) X (HxW)\n","        \"\"\"\n","        m_batchsize, C, height, width = x.size()\n","        proj_query = self.query_conv(x).view(m_batchsize, -1, width * height).permute(0, 2, 1)\n","        proj_key = self.key_conv(x).view(m_batchsize, -1, width * height)\n","        energy = torch.bmm(proj_query, proj_key)\n","        attention = self.softmax(energy)\n","        proj_value = self.value_conv(x).view(m_batchsize, -1, width * height)\n","\n","        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n","        out = out.view(m_batchsize, C, height, width)\n","\n","        out = self.gamma * out + x\n","        return out\n","\n","\n","class CAM_Module(Module):\n","    \"\"\" Channel attention module\"\"\"\n","\n","    def __init__(self, in_dim):\n","        super(CAM_Module, self).__init__()\n","        self.chanel_in = in_dim\n","\n","        self.gamma = Parameter(torch.zeros(1))\n","        self.softmax = Softmax(dim=-1)\n","\n","    def forward(self, x):\n","        \"\"\"\n","            inputs :\n","                x : input feature maps( B X C X H X W)\n","            returns :\n","                out : attention value + input feature\n","                attention: B X C X C\n","        \"\"\"\n","        m_batchsize, C, height, width = x.size()\n","        proj_query = x.view(m_batchsize, C, -1)\n","        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)\n","        energy = torch.bmm(proj_query, proj_key)\n","        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy) - energy\n","        attention = self.softmax(energy_new)\n","        proj_value = x.view(m_batchsize, C, -1)\n","\n","        out = torch.bmm(attention, proj_value)\n","        out = out.view(m_batchsize, C, height, width)\n","\n","        out = self.gamma * out + x\n","        return out\n"],"metadata":{"id":"vuVeQnJmgclk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","\n","class ESPNetDecoder():\n","    def __init__(self):\n","\n","        # light-weight decoder\n","        self.level3_C = C(128 + 3, classes, 1, 1)\n","        self.br = nn.BatchNorm2d(classes, eps=1e-03)\n","        self.conv = CBR(19 + classes, classes, 3, 1)\n","\n","        self.up_l3 = nn.Sequential(\n","            nn.ConvTranspose2d(classes, classes, 2, stride=2, padding=0, output_padding=0, bias=False))\n","        self.combine_l2_l3 = nn.Sequential(BR(2 * classes),\n","                                           DilatedParllelResidualBlockB(2 * classes, classes, add=False))\n","\n","        self.up_l2 = nn.Sequential(\n","            nn.ConvTranspose2d(classes, classes, 2, stride=2, padding=0, output_padding=0, bias=False), BR(classes))\n","\n","        self.classifier = nn.ConvTranspose2d(classes, classes, 2, stride=2, padding=0, output_padding=0, bias=False)\n","\n","    def forward(self, input):\n","        '''\n","        :param input: RGB image\n","        :return: transformed feature map\n","        '''\n","        output0 = self.modules[0](input)\n","        inp1 = self.modules[1](input)\n","        inp2 = self.modules[2](input)\n","\n","        output0_cat = self.modules[3](torch.cat([output0, inp1], 1))\n","        output1_0 = self.modules[4](output0_cat)  # down-sampled\n","\n","        for i, layer in enumerate(self.modules[5]):\n","            if i == 0:\n","                output1 = layer(output1_0)\n","            else:\n","                output1 = layer(output1)\n","\n","        output1_cat = self.modules[6](torch.cat([output1, output1_0, inp2], 1))\n","\n","        output2_0 = self.modules[7](output1_cat)  # down-sampled\n","        for i, layer in enumerate(self.modules[8]):\n","            if i == 0:\n","                output2 = layer(output2_0)\n","            else:\n","                output2 = layer(output2)\n","\n","        output2_cat = self.modules[9](torch.cat([output2_0, output2], 1))  # concatenate for feature map width expansion\n","\n","        output2_c = self.up_l3(self.br(self.modules[10](output2_cat)))  # RUM\n","\n","        output1_C = self.level3_C(output1_cat)  # project to C-dimensional space\n","        comb_l2_l3 = self.up_l2(self.combine_l2_l3(torch.cat([output1_C, output2_c], 1)))  # RUM\n","\n","        concat_features = self.conv(torch.cat([comb_l2_l3, output0_cat], 1))\n","\n","        classifier = self.classifier(concat_features)\n","        return classifier\n","\n","\n","class ENetDecoder():\n","    def __init__(self):\n","        # Stage 4 - Decoder\n","        self.upsample4_0 = UpsamplingBottleneck(\n","            128, 64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n","        self.regular4_1 = RegularBottleneck(\n","            64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n","        self.regular4_2 = RegularBottleneck(\n","            64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n","\n","        # Stage 5 - Decoder\n","        self.upsample5_0 = UpsamplingBottleneck(\n","            64, 16, padding=1, dropout_prob=0.1, relu=decoder_relu)\n","        self.regular5_1 = RegularBottleneck(\n","            16, padding=1, dropout_prob=0.1, relu=decoder_relu)\n","        self.transposed_conv = nn.ConvTranspose2d(\n","            16,\n","            num_classes,\n","            kernel_size=3,\n","            stride=2,\n","            padding=1,\n","            output_padding=1,\n","            bias=False)\n","\n","    def forward(self, x):\n","        # Initial block\n","        x = self.initial_block(x)\n","\n","        # Stage 1 - Encoder\n","        x, max_indices1_0 = self.downsample1_0(x)\n","        x = self.regular1_1(x)\n","        x = self.regular1_2(x)\n","        x = self.regular1_3(x)\n","        x = self.regular1_4(x)\n","\n","        # Stage 2 - Encoder\n","        x, max_indices2_0 = self.downsample2_0(x)\n","        x = self.regular2_1(x)\n","        x = self.dilated2_2(x)\n","        x = self.asymmetric2_3(x)\n","        x = self.dilated2_4(x)\n","        x = self.regular2_5(x)\n","        x = self.dilated2_6(x)\n","        x = self.asymmetric2_7(x)\n","        x = self.dilated2_8(x)\n","\n","        # Stage 3 - Encoder\n","        x = self.regular3_0(x)\n","        x = self.dilated3_1(x)\n","        x = self.asymmetric3_2(x)\n","        x = self.dilated3_3(x)\n","        x = self.regular3_4(x)\n","        x = self.dilated3_5(x)\n","        x = self.asymmetric3_6(x)\n","        x = self.dilated3_7(x)\n","\n","        # Stage 4 - Decoder\n","        x = self.upsample4_0(x, max_indices2_0)\n","        x = self.regular4_1(x)\n","        x = self.regular4_2(x)\n","\n","        # Stage 5 - Decoder\n","        x = self.upsample5_0(x, max_indices1_0)\n","        x = self.regular5_1(x)\n","        x = self.transposed_conv(x)\n","\n","        return x\n","\n","\n","class FCNDecoder(nn.Module):\n","    def __init__(self, decode_layers, decode_channels=[], decode_last_stride=8):\n","        super(FCNDecoder, self).__init__()\n","\n","        self._decode_channels = [512, 256]\n","        self._out_channel = 64\n","        self._decode_layers = decode_layers\n","\n","        self._conv_layers = []\n","        for _ch in self._decode_channels:\n","            self._conv_layers.append(nn.Conv2d(_ch, self._out_channel, kernel_size=1, bias=False).to(DEVICE))\n","\n","        self._conv_final = nn.Conv2d(self._out_channel, 2, kernel_size=1, bias=False)\n","        self._deconv = nn.ConvTranspose2d(self._out_channel, self._out_channel, kernel_size=4, stride=2, padding=1,\n","                                          bias=False)\n","\n","        self._deconv_final = nn.ConvTranspose2d(self._out_channel, self._out_channel, kernel_size=16,\n","                                                stride=decode_last_stride,\n","                                                padding=4, bias=False)\n","\n","    def forward(self, encode_data):\n","        ret = {}\n","        input_tensor = encode_data[self._decode_layers[0]]\n","        input_tensor.to(DEVICE)\n","        score = self._conv_layers[0](input_tensor)\n","        for i, layer in enumerate(self._decode_layers[1:]):\n","            deconv = self._deconv(score)\n","\n","            input_tensor = encode_data[layer]\n","            score = self._conv_layers[i](input_tensor)\n","\n","            fused = torch.add(deconv, score)\n","            score = fused\n","\n","        deconv_final = self._deconv_final(score)\n","        score_final = self._conv_final(deconv_final)\n","\n","        ret['logits'] = score_final\n","        ret['deconv'] = deconv_final\n","        return ret"],"metadata":{"id":"ndhUxk3rgofx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# coding: utf-8\n","\"\"\"\n","Shared encoders (U-net).\n","\"\"\"\n","from functools import partial\n","\n","class VGGEncoder(nn.Module):\n","    \"\"\"\n","    Simple VGG Encoder\n","    \"\"\"\n","\n","    def __init__(self, num_blocks, in_channels, out_channels):\n","        super(VGGEncoder, self).__init__()\n","\n","        self.pretrained_modules = models.vgg16(pretrained=True).features\n","\n","        self.num_blocks = num_blocks\n","        self._in_channels = in_channels\n","        self._out_channels = out_channels\n","        self._conv_reps = [2, 2, 3, 3, 3]\n","        self.net = nn.Sequential()\n","        self.pretrained_net = nn.Sequential()\n","\n","        for i in range(num_blocks):\n","            self.net.add_module(\"block\" + str(i + 1), self._encode_block(i + 1))\n","            self.pretrained_net.add_module(\"block\" + str(i + 1), self._encode_pretrained_block(i + 1))\n","\n","    def _encode_block(self, block_id, kernel_size=3, stride=1):\n","        out_channels = self._out_channels[block_id - 1]\n","        padding = (kernel_size - 1) // 2\n","        seq = nn.Sequential()\n","\n","        for i in range(self._conv_reps[block_id - 1]):\n","            if i == 0:\n","                in_channels = self._in_channels[block_id - 1]\n","            else:\n","                in_channels = out_channels\n","            seq.add_module(\"conv_{}_{}\".format(block_id, i + 1),\n","                           nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding))\n","            seq.add_module(\"bn_{}_{}\".format(block_id, i + 1), nn.BatchNorm2d(out_channels))\n","            seq.add_module(\"relu_{}_{}\".format(block_id, i + 1), nn.ReLU())\n","        seq.add_module(\"maxpool\" + str(block_id), nn.MaxPool2d(kernel_size=2, stride=2))\n","        return seq\n","\n","    def _encode_pretrained_block(self, block_id):\n","        seq = nn.Sequential()\n","        for i in range(0, self._conv_reps[block_id - 1], 4):\n","            seq.add_module(\"conv_{}_{}\".format(block_id, i + 1), self.pretrained_modules[i])\n","            seq.add_module(\"relu_{}_{}\".format(block_id, i + 2), self.pretrained_modules[i + 1])\n","            seq.add_module(\"conv_{}_{}\".format(block_id, i + 3), self.pretrained_modules[i + 2])\n","            seq.add_module(\"relu_{}_{}\".format(block_id, i + 4), self.pretrained_modules[i + 3])\n","            seq.add_module(\"maxpool\" + str(block_id), self.pretrained_modules[i + 4])\n","        return seq\n","\n","    def forward(self, input_tensor):\n","        ret = OrderedDict()\n","        # 5 stage of encoding\n","        X = input_tensor\n","        for i, block in enumerate(self.net):\n","            pool = block(X)\n","            ret[\"pool\" + str(i + 1)] = pool\n","\n","            X = pool\n","        return ret\n","\n","\n","class ESPNetEncoder(nn.Module):\n","    \"\"\"\n","    ESPNet-C encoder\n","    \"\"\"\n","\n","    def __init__(self, classes=20, p=5, q=3):\n","        '''\n","        :param classes: number of classes in the dataset. Default is 20 for the cityscapes\n","        :param p: depth multiplier\n","        :param q: depth multiplier\n","        '''\n","        super().__init__()\n","        self.level1 = CBR(3, 16, 3, 2)\n","        self.sample1 = InputProjectionA(1)\n","        self.sample2 = InputProjectionA(2)\n","\n","        self.b1 = BR(16 + 3)\n","        self.level2_0 = DownSamplerB(16 + 3, 64)\n","\n","        self.level2 = nn.ModuleList()\n","        for i in range(0, p):\n","            self.level2.append(DilatedParallelResidualBlockB(64, 64))\n","        self.b2 = BR(128 + 3)\n","\n","        self.level3_0 = DownSamplerB(128 + 3, 128)\n","        self.level3 = nn.ModuleList()\n","        for i in range(0, q):\n","            self.level3.append(DilatedParallelResidualBlockB(128, 128))\n","        self.b3 = BR(256)\n","\n","        self.classifier = C(256, classes, 1, 1)\n","\n","    def forward(self, input):\n","        '''\n","        :param input: Receives the input RGB image\n","        :return: the transformed feature map with spatial dimensions 1/8th of the input image\n","        '''\n","        output0 = self.level1(input)\n","        inp1 = self.sample1(input)\n","        inp2 = self.sample2(input)\n","\n","        output0_cat = self.b1(torch.cat([output0, inp1], 1))\n","        output1_0 = self.level2_0(output0_cat)  # down-sampled\n","\n","        for i, layer in enumerate(self.level2):\n","            if i == 0:\n","                output1 = layer(output1_0)\n","            else:\n","                output1 = layer(output1)\n","\n","        output1_cat = self.b2(torch.cat([output1, output1_0, inp2], 1))\n","\n","        output2_0 = self.level3_0(output1_cat)  # down-sampled\n","        for i, layer in enumerate(self.level3):\n","            if i == 0:\n","                output2 = layer(output2_0)\n","            else:\n","                output2 = layer(output2)\n","\n","        output2_cat = self.b3(torch.cat([output2_0, output2], 1))\n","\n","        classifier = self.classifier(output2_cat)\n","\n","        return classifier\n","\n","\n","class ENetEncoder(nn.Module):\n","    \"\"\"\n","    ENET Encoder\n","    \"\"\"\n","\n","    def __init__(self, num_classes, encoder_relu=False, decoder_relu=True):\n","        super().__init__()\n","\n","    def forward(self, input):\n","        self.initial_block = InitialBlock(3, 16, padding=1, relu=encoder_relu)\n","\n","        # Stage 1 - Encoder\n","        self.downsample1_0 = DownsamplingBottleneck(16, 64, padding=1, return_indices=True, dropout_prob=0.01,\n","                                                    relu=encoder_relu)\n","        self.regular1_1 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n","        self.regular1_2 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n","        self.regular1_3 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n","        self.regular1_4 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n","\n","        # Stage 2 - Encoder\n","        self.downsample2_0 = DownsamplingBottleneck(64, 128, padding=1, return_indices=True, dropout_prob=0.1,\n","                                                    relu=encoder_relu)\n","        self.regular2_1 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n","        self.dilated2_2 = RegularBottleneck(128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n","        self.asymmetric2_3 = RegularBottleneck(128, kernel_size=5, padding=2, asymmetric=True, dropout_prob=0.1,\n","                                               relu=encoder_relu)\n","        self.dilated2_4 = RegularBottleneck(128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n","        self.regular2_5 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n","        self.dilated2_6 = RegularBottleneck(128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n","        self.asymmetric2_7 = RegularBottleneck(128, kernel_size=5, asymmetric=True, padding=2, dropout_prob=0.1,\n","                                               relu=encoder_relu)\n","        self.dilated2_8 = RegularBottleneck(128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n","\n","        # Stage 3 - Encoder\n","        self.regular3_0 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n","        self.dilated3_1 = RegularBottleneck(128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n","        self.asymmetric3_2 = RegularBottleneck(128, kernel_size=5, padding=2, asymmetric=True, dropout_prob=0.1,\n","                                               relu=encoder_relu)\n","        self.dilated3_3 = RegularBottleneck(128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n","        self.regular3_4 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n","        self.dilated3_5 = RegularBottleneck(128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n","        self.asymmetric3_6 = RegularBottleneck(128, kernel_size=5, asymmetric=True, padding=2, dropout_prob=0.1,\n","                                               relu=encoder_relu)\n","        self.dilated3_7 = RegularBottleneck(128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)"],"metadata":{"id":"6ZfN67E4gxke"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class AverageMeter():\n","    \"\"\"Computes and stores the average and current value\n","       Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"metadata":{"id":"xle7vmlMhV6V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","From https://github.com/nyoki-mtl/pytorch-discriminative-loss/blob/master/src/loss.py\n","This is the implementation of following paper:\n","https://arxiv.org/pdf/1802.05591.pdf\n","This implementation is based on following code:\n","https://github.com/Wizaron/instance-segmentation-pytorch\n","\"\"\"\n","\n","from torch.nn.modules.loss import _Loss\n","\n","\n","DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","\n","class DiscriminativeLoss(_Loss):\n","\n","    def __init__(self, delta_var=0.5, delta_dist=1.5, norm=2, alpha=1.0, beta=1.0, gamma=0.001,\n","                 usegpu=False, size_average=True):\n","        super(DiscriminativeLoss, self).__init__(reduction='mean')\n","        self.delta_var = delta_var\n","        self.delta_dist = delta_dist\n","        self.norm = norm\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.gamma = gamma\n","        self.usegpu = usegpu\n","        assert self.norm in [1, 2]\n","\n","    def forward(self, input, target):\n","        # _assert_no_grad(target)\n","        return self._discriminative_loss(input, target)\n","\n","    def _discriminative_loss(self, embedding, seg_gt):\n","        batch_size = embedding.shape[0]\n","        embed_dim = embedding.shape[1]\n","\n","        var_loss = torch.tensor(0, dtype=embedding.dtype, device=embedding.device)\n","        dist_loss = torch.tensor(0, dtype=embedding.dtype, device=embedding.device)\n","        reg_loss = torch.tensor(0, dtype=embedding.dtype, device=embedding.device)\n","\n","        for b in range(batch_size):\n","            embedding_b = embedding[b]  # (embed_dim, H, W)\n","            seg_gt_b = seg_gt[b]\n","\n","            labels = torch.unique(seg_gt_b)\n","            labels = labels[labels != 0]\n","            num_lanes = len(labels)\n","            if num_lanes == 0:\n","                # please refer to issue here: https://github.com/harryhan618/LaneNet/issues/12\n","                _nonsense = embedding.sum()\n","                _zero = torch.zeros_like(_nonsense)\n","                var_loss = var_loss + _nonsense * _zero\n","                dist_loss = dist_loss + _nonsense * _zero\n","                reg_loss = reg_loss + _nonsense * _zero\n","                continue\n","\n","            centroid_mean = []\n","            for lane_idx in labels:\n","                seg_mask_i = (seg_gt_b == lane_idx)\n","                if not seg_mask_i.any():\n","                    continue\n","                embedding_i = embedding_b[seg_mask_i]\n","\n","                mean_i = torch.mean(embedding_i, dim=0)\n","                centroid_mean.append(mean_i)\n","\n","                # ---------- var_loss -------------\n","                var_loss = var_loss + torch.mean(F.relu(\n","                    torch.norm(embedding_i - mean_i, dim=1) - self.delta_var) ** 2) / num_lanes\n","            centroid_mean = torch.stack(centroid_mean)  # (n_lane, embed_dim)\n","\n","            if num_lanes > 1:\n","                centroid_mean1 = centroid_mean.reshape(-1, 1, embed_dim)\n","                centroid_mean2 = centroid_mean.reshape(1, -1, embed_dim)\n","                dist = torch.norm(centroid_mean1 - centroid_mean2, dim=2)  # shape (num_lanes, num_lanes)\n","                dist = dist + torch.eye(num_lanes, dtype=dist.dtype,\n","                                        device=dist.device) * self.delta_dist  # diagonal elements are 0, now mask above delta_d\n","\n","                # divided by two for double calculated loss above, for implementation convenience\n","                dist_loss = dist_loss + torch.sum(F.relu(-dist + self.delta_dist) ** 2) / (\n","                        num_lanes * (num_lanes - 1)) / 2\n","\n","            # reg_loss is not used in original paper\n","            # reg_loss = reg_loss + torch.mean(torch.norm(centroid_mean, dim=1))\n","\n","        var_loss = var_loss / batch_size\n","        dist_loss = dist_loss / batch_size\n","        reg_loss = reg_loss / batch_size\n","        return var_loss, dist_loss, reg_loss\n","\n","\n","class HNetLoss(_Loss):\n","    \"\"\"\n","    HNet Loss\n","    \"\"\"\n","\n","    def __init__(self, gt_pts, transformation_coefficient, name, usegpu=True):\n","        \"\"\"\n","        :param gt_pts: [x, y, 1]\n","        :param transformation_coeffcient: [[a, b, c], [0, d, e], [0, f, 1]]\n","        :param name:\n","        :return: \n","        \"\"\"\n","        super(HNetLoss, self).__init__()\n","\n","        self.gt_pts = gt_pts\n","\n","        self.transformation_coefficient = transformation_coefficient\n","        self.name = name\n","        self.usegpu = usegpu\n","\n","    def _hnet_loss(self):\n","        \"\"\"\n","        :return:\n","        \"\"\"\n","        H, preds = self._hnet()\n","        x_transformation_back = torch.matmul(torch.inverse(H), preds)\n","        loss = torch.mean(torch.pow(self.gt_pts.t()[0, :] - x_transformation_back[0, :], 2))\n","\n","        return loss\n","\n","    def _hnet(self):\n","        \"\"\"\n","        :return:\n","        \"\"\"\n","        self.transformation_coefficient = torch.cat((self.transformation_coefficient, torch.tensor([1.0])),\n","                                                    dim=0)\n","        H_indices = torch.tensor([0, 1, 2, 4, 5, 7, 8])\n","        H_shape = 9\n","        H = torch.zeros(H_shape)\n","        H.scatter_(dim=0, index=H_indices, src=self.transformation_coefficient)\n","        H = H.view((3, 3))\n","\n","        pts_projects = torch.matmul(H, self.gt_pts.t())\n","\n","        Y = pts_projects[1, :]\n","        X = pts_projects[0, :]\n","        Y_One = torch.ones(Y.size())\n","        Y_stack = torch.stack((torch.pow(Y, 3), torch.pow(Y, 2), Y, Y_One), dim=1).squeeze()\n","        w = torch.matmul(torch.matmul(torch.inverse(torch.matmul(Y_stack.t(), Y_stack)),\n","                                      Y_stack.t()),\n","                         X.view(-1, 1))\n","\n","        x_preds = torch.matmul(Y_stack, w)\n","        preds = torch.stack((x_preds.squeeze(), Y, Y_One), dim=1).t()\n","        return (H, preds)\n","\n","    def _hnet_transformation(self):\n","        \"\"\"\n","        \"\"\"\n","        H, preds = self._hnet()\n","        x_transformation_back = torch.matmul(torch.inverse(H), preds)\n","\n","        return x_transformation_back\n","\n","    def forward(self, input, target, n_clusters):\n","        return self._hnet_loss(input, target)"],"metadata":{"id":"_6MDL4HBhCbE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# coding: utf-8\n","\"\"\"\n","LaneNet model\n","https://arxiv.org/pdf/1807.01726.pdf\n","\"\"\"\n","\n","DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","\n","class LaneNet(nn.Module):\n","    def __init__(self, arch=\"VGG\"):\n","        super(LaneNet, self).__init__()\n","        # no of instances for segmentation\n","        self.no_of_instances = 5\n","        encode_num_blocks = 5\n","        in_channels = [3, 64, 128, 256, 512]\n","        out_channels = in_channels[1:] + [512]\n","        self._arch = arch\n","        if self._arch == 'VGG':\n","            self._encoder = VGGEncoder(encode_num_blocks, in_channels, out_channels)\n","            self._encoder.to(DEVICE)\n","\n","            decode_layers = [\"pool5\", \"pool4\", \"pool3\"]\n","            decode_channels = out_channels[:-len(decode_layers) - 1:-1]\n","            decode_last_stride = 8\n","            self._decoder = FCNDecoder(decode_layers, decode_channels, decode_last_stride)\n","            self._decoder.to(DEVICE)\n","        elif self._arch == 'ESPNet':\n","            raise NotImplementedError\n","        elif self._arch == 'ENNet':\n","            raise NotImplementedError\n","\n","        self._pix_layer = nn.Conv2d(in_channels=64, out_channels=self.no_of_instances, kernel_size=1, bias=False).to(\n","            DEVICE)\n","        self.relu = nn.ReLU().to(DEVICE)\n","\n","    def forward(self, input_tensor):\n","        encode_ret = self._encoder(input_tensor)\n","        decode_ret = self._decoder(encode_ret)\n","\n","        decode_logits = decode_ret['logits']\n","\n","        decode_logits = decode_logits.to(DEVICE)\n","\n","        binary_seg_ret = torch.argmax(F.softmax(decode_logits, dim=1), dim=1, keepdim=True)\n","\n","        decode_deconv = decode_ret['deconv']\n","        pix_embedding = self.relu(self._pix_layer(decode_deconv))\n","\n","        return {\n","            'instance_seg_logits': pix_embedding,\n","            'binary_seg_pred': binary_seg_ret,\n","            'binary_seg_logits': decode_logits\n","        }\n","\n","\n","def compute_loss(net_output, binary_label, instance_label):\n","    k_binary = 0.7\n","    k_instance = 0.3\n","    k_dist = 1.0\n","\n","    ce_loss_fn = nn.CrossEntropyLoss()\n","    binary_seg_logits = net_output[\"binary_seg_logits\"]\n","\n","    print(binary_seg_logits.shape, binary_label.shape)\n","\n","    binary_loss = ce_loss_fn(binary_seg_logits, binary_label)\n","\n","    pix_embedding = net_output[\"instance_seg_logits\"]\n","    ds_loss_fn = DiscriminativeLoss(0.5, 1.5, 1.0, 1.0, 0.001)\n","    print(pix_embedding.shape, instance_label.shape)\n","    var_loss, dist_loss, reg_loss = ds_loss_fn(pix_embedding, instance_label)\n","    binary_loss = binary_loss * k_binary\n","    instance_loss = var_loss * k_instance\n","    dist_loss = dist_loss * k_dist\n","    total_loss = binary_loss + instance_loss + dist_loss\n","    out = net_output[\"binary_seg_pred\"]\n","    iou = 0\n","    batch_size = out.size()[0]\n","    for i in range(batch_size):\n","        PR = out[i].squeeze(0).nonzero().size()[0]\n","        GT = binary_label[i].nonzero().size()[0]\n","        TP = (out[i].squeeze(0) * binary_label[i]).nonzero().size()[0]\n","        union = PR + GT - TP\n","        iou += TP / union\n","    iou = iou / batch_size\n","    return total_loss, binary_loss, instance_loss, out, iou"],"metadata":{"id":"CUcjI6hSm_pV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import os\n","import sys\n","from tqdm import tqdm\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","from torchvision import transforms\n","\n","\n","# might want this in the transformer part as well\n","VGG_MEAN = [103.939, 116.779, 123.68]\n","DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","\n","def compose_img(image_data, out, binary_label, pix_embedding, instance_label, i):\n","    val_gt = (image_data[i].cpu().numpy().transpose(1, 2, 0) + VGG_MEAN).astype(np.uint8)\n","    val_pred = out[i].squeeze(0).cpu().numpy().transpose(0, 1) * 255\n","    val_label = binary_label[i].squeeze(0).cpu().numpy().transpose(0, 1) * 255\n","    val_out = np.zeros((val_pred.shape[0], val_pred.shape[1], 3), dtype=np.uint8)\n","    val_out[:, :, 0] = val_pred\n","    val_out[:, :, 1] = val_label\n","    val_gt[val_out == 255] = 255\n","    # epsilon = 1e-5\n","    # pix_embedding = pix_embedding[i].data.cpu().numpy()\n","    # pix_vec = pix_embedding / (np.sum(pix_embedding, axis=0, keepdims=True) + epsilon) * 255\n","    # pix_vec = np.round(pix_vec).astype(np.uint8).transpose(1, 2, 0)\n","    # ins_label = instance_label[i].data.cpu().numpy().transpose(0, 1)\n","    # ins_label = np.repeat(np.expand_dims(ins_label, -1), 3, -1)\n","    # val_img = np.concatenate((val_gt, pix_vec, ins_label), axis=0)\n","    # val_img = np.concatenate((val_gt, pix_vec), axis=0)\n","    # return val_img\n","    return val_gt\n","\n","def train(train_loader, model, optimizer, epoch):\n","    batch_time = AverageMeter()\n","    mean_iou = AverageMeter()\n","    total_losses = AverageMeter()\n","    binary_losses = AverageMeter()\n","    instance_losses = AverageMeter()\n","    end = time.time()\n","    step = 0\n","\n","    t = tqdm(enumerate(iter(train_loader)), leave=False, total=len(train_loader))\n","\n","    for batch_idx, batch in t:\n","        step += 1\n","        image_data = Variable(batch[0]).type(torch.FloatTensor).to(DEVICE)\n","        print(image_data.shape)\n","        binary_label = Variable(batch[1]).type(torch.LongTensor).to(DEVICE)\n","        instance_label = Variable(batch[2]).type(torch.FloatTensor).to(DEVICE)\n","\n","        # forward pass\n","        net_output = model(image_data)\n","\n","        # compute loss\n","        total_loss, binary_loss, instance_loss, out, train_iou = compute_loss(net_output, binary_label, instance_label)\n","\n","        # update loss in AverageMeter instance\n","        total_losses.update(total_loss.item(), image_data.size()[0])\n","        binary_losses.update(binary_loss.item(), image_data.size()[0])\n","        instance_losses.update(instance_loss.item(), image_data.size()[0])\n","        mean_iou.update(train_iou, image_data.size()[0])\n","\n","        # reset gradients\n","        optimizer.zero_grad()\n","\n","        # backpropagate\n","        total_loss.backward()\n","\n","        # update weights\n","        optimizer.step()\n","\n","        # update batch time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","\n","        if step % 500 == 0:\n","            print(\n","                \"Epoch {ep} Step {st} |({batch}/{size})| ETA: {et:.2f}|Total loss:{tot:.5f}|Binary loss:{bin:.5f}|Instance loss:{ins:.5f}|IoU:{iou:.5f}\".format(\n","                    ep=epoch + 1,\n","                    st=step,\n","                    batch=batch_idx + 1,\n","                    size=len(train_loader),\n","                    et=batch_time.val,\n","                    tot=total_losses.avg,\n","                    bin=binary_losses.avg,\n","                    ins=instance_losses.avg,\n","                    iou=train_iou,\n","                ))\n","            sys.stdout.flush()\n","            train_img_list = []\n","            for i in range(3):\n","                train_img_list.append(\n","                    compose_img(image_data, out, binary_label, net_output[\"instance_seg_logits\"], instance_label, i))\n","            train_img = np.concatenate(train_img_list, axis=1)\n","            cv2.imwrite(os.path.join(\"./output\", \"train_\" + str(epoch + 1) + \"_step_\" + str(step) + \".png\"), train_img)\n","    return mean_iou.avg\n","\n","\n","def save_model(save_path, epoch, model):\n","    save_name = os.path.join(save_path, f'{epoch}_checkpoint.pth')\n","    torch.save(model, save_name)\n","    print(\"model is saved: {}\".format(save_name))\n","\n","\n","# Main\n","save_path = \"./checkpoints\"\n","\n","if not os.path.isdir(save_path):\n","    os.makedirs(save_path)\n","\n","train_dataset_file = os.path.join('/content/drive/MyDrive/training/', 'train.txt')\n","\n","train_dataset = LaneDataSet(train_dataset_file, transform=transforms.Compose([Rescale((512, 256))]))\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","model = LaneNet()\n","model.to(DEVICE)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n","print(f\"{10} epochs {len(train_dataset)} training samples\\n\")\n","\n","for epoch in range(0, 10):\n","    print(f\"Epoch {epoch}\")\n","    train_iou = train(train_loader, model, optimizer, epoch)\n","    if args.val:\n","        val_iou = test(val_loader, model, epoch)\n","    if (epoch + 1) % 5 == 0:\n","        save_model(save_path, epoch, model)\n","\n","    print(f\"Train IoU : {train_iou}\")\n","    if args.val:\n","        print(f\"Val IoU : {val_iou}\")"],"metadata":{"id":"Luwxw3daeZLU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ahR8y9zueZYD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qBvHKWcgeZbu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"38nidLJNeZes"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qquuMfHweZid"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VMJwanw5eZlp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1pvMlxKfeZr8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"o8elIYHXeZxm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E7xQ2Zu4eZ2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DoubleConv(nn.Module):\n","\n","  def __init__(self, in_channels, out_channels):\n","    super(DoubleConv, self).__init__()\n","\n","    self.conv = nn.Sequential(\n","         nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n","         nn.BatchNorm2d(out_channels),\n","         nn.ReLU(inplace=True),\n","         nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n","         nn.BatchNorm2d(out_channels),\n","         nn.ReLU(inplace=True),\n","     )\n","\n","  def forward(self, x):\n","    return self.conv(x)\n","\n","\n","class UNET(nn.Module):\n","  def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n","    super(UNET, self).__init__()\n","\n","    self.downs = nn.ModuleList()\n","    self.ups = nn.ModuleList()\n","    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    # Down part\n","    for feature in features:\n","      self.downs.append(DoubleConv(in_channels, feature))\n","      in_channels = feature\n","\n","    # Up part\n","    for feature in reversed(features):\n","      self.ups.append(\n","          nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2),\n","      )\n","      self.ups.append(DoubleConv(feature*2, feature))\n","\n","    self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n","    self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n","\n","  def forward(self, x):\n","    skip_connections = []\n","\n","    for down in self.downs:\n","      x = down(x)\n","      skip_connections.append(x)\n","\n","    x = self.pool(x)\n","    x = self.bottleneck(x)\n","    skip_connections = skip_connections[::-1]\n","\n","    for ind in range(0, len(self.ups), 2):\n","      x = self.ups[ind](x)\n","      skip_connection = skip_connections[ind//2]\n","\n","      if x.shape != skip_connection.shape:\n","        x = TF.resize(x, skip_connection.shape[2:])\n","\n","      skip_concat = torch.cat((skip_connection, x), dim=1)\n","      x = self.ups[ind+1](skip_concat)\n","\n","    return self.final_conv(x)\n"],"metadata":{"id":"Z_QRlYuxWQk_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test():\n","  x = torch.randn((3, 1, 161, 161))\n","  model = UNET(in_channels=1, out_channels=1)\n","  preds = model(x)\n","  print(x.shape, preds.shape)\n"],"metadata":{"id":"GOLrhQfGi6eo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test()"],"metadata":{"id":"4xINBy7_jyUJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATA_PATH = '/content/drive/MyDrive/'\n","\n","train_file = 'label_data_0313.json'\n","val_file = 'label_data_0531.json'\n","test_file = 'label_data_0601.json'\n","\n","DEFAULT_SIZE = (256, 512)"],"metadata":{"id":"wgy21Zi1w4Fq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LaneDataset(torch.utils.data.Dataset):\n","  def __init__(self, dataset_path, train=True, size=DEFAULT_SIZE):\n"],"metadata":{"id":"EIvdER1TrBcR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qhn0PpTDrBkn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_path = '/content/drive/MyDrive/'\n","\n","train_info =  data_path + 'label_data_0313.json'\n","val_info = data_path + 'label_data_0531.json'\n","test_info = data_path + 'label_data_0601.json'\n"],"metadata":{"id":"mjah8QDhs-bz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# read each line of json file\n","json_gt = [json.loads(line) for line in open(train_info)]\n","gt = json_gt[0]\n","gt_lanes = gt['lanes']\n","y_samples = gt['h_samples']\n","raw_file = data_path + gt['raw_file']\n","\n","print(raw_file)\n","\n","img = cv2.imread(raw_file)\n","print(img.shape)\n","cv2_imshow(cv2.resize(img, (512, 256), interpolation=cv2.INTER_LINEAR))"],"metadata":{"id":"MAOAwP1wtrmO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gt_lanes_vis = [[(x, y) for (x, y) in zip(lane, y_samples)\n","                  if x >= 0] for lane in gt_lanes]\n","img_vis = img.copy()\n","\n","for lane in gt_lanes_vis:\n","    img2 = cv2.polylines(img_vis, np.int32([lane]), isClosed=False,\n","                   color=(0,255,0), thickness=5)\n","    \n","img2 = cv2.resize(img2, (512, 256), interpolation=cv2.INTER_LINEAR)\n","cv2_imshow(img2)\n","print(img2.shape, img.dtype)\n","    "],"metadata":{"id":"TgGfukZKwK0N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mask = np.zeros_like(img)\n","print(img.shape, mask.shape)\n","colors = [[255,0,0],[0,255,0],[0,0,255],[0,255,255]]\n","for i in range(len(gt_lanes_vis)):\n","    cv2.polylines(mask, np.int32([gt_lanes_vis[i]]), isClosed=False,color=colors[i], thickness=5)\n","\n","cv2_imshow(cv2.resize(mask, (512, 256), interpolation=cv2.INTER_LINEAR))\n","\n","# create grey-scale label image\n","label = np.zeros((720,1280),dtype = np.uint8)\n","for i in range(len(colors)):\n","   label[np.where((mask == colors[i]).all(axis = 2))] = i+1\n","\n","print(label.shape)"],"metadata":{"id":"bQpV911O4KyW"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E8qsezviNKC2"},"source":["DEFAULT_SIZE = (256, 512)\n","IMG_SIZE = (720, 1280)\n","\n","import torch\n","\n","class LaneDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataset_path, train=True, size=DEFAULT_SIZE):\n","        self.img_size = size\n","        self.train_flag = train\n","        self.data = []\n","        self.data_path = dataset_path\n","\n","        if train:\n","          self.file_path = data_path + 'label_data_0313.json'\n","        else:\n","          self.file_path = data_path + 'label_data_0601.json'\n","\n","        json_gt = [json.loads(line) for line in open(self.file_path)]\n","\n","        for i in range(len(json_gt)):\n","          if i%100 == 0:\n","            print(\"%i in %i\" % (i, len(json_gt)))\n","          one_sample = dict()\n","\n","          data_item = json_gt[i]\n","          gt_lanes = data_item['lanes']\n","          y_samples = data_item['h_samples']\n","          raw_file = self.data_path + data_item['raw_file']\n","\n","          # get image\n","          img = cv2.imread(raw_file)\n","          img_resized = cv2.resize(img, (self.img_size[1], self.img_size[0]), interpolation=cv2.INTER_LINEAR)\n","          one_sample['image'] = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n","\n","          # get binary gt\n","          mask = np.zeros((720,1280),dtype = np.uint8)\n","\n","          gt_lanes_vis = [[(x, y) for (x, y) in zip(lane, y_samples)\n","                  if x >= 0] for lane in gt_lanes]\n","          for lane in gt_lanes_vis:\n","               cv2.polylines(mask, np.int32([lane]), isClosed=False,\n","                   color=(1), thickness=5)\n","          one_sample['bin_gt'] = cv2.resize(mask, (self.img_size[1], self.img_size[0]), interpolation=cv2.INTER_LINEAR)\n","\n","          # get segmentation gt\n","          mask = np.zeros((720,1280, 3),dtype = np.uint8)\n","          colors = [[255,0,0],[0,255,0],[0,0,255],[0,255,255]]\n","          for i in range(len(gt_lanes_vis)):\n","              cv2.polylines(mask, np.int32([gt_lanes_vis[i]]), isClosed=False,color=colors[i], thickness=5)\n","\n","          label = np.zeros((720,1280),dtype = np.uint8)\n","          for i in range(len(colors)):\n","            label[np.where((mask == colors[i]).all(axis = 2))] = i+1\n","\n","          one_sample['seg_img'] = cv2.resize(label, (self.img_size[1], self.img_size[0]), interpolation=cv2.INTER_LINEAR)\n","\n","        self.data.append(one_sample)\n","\n","    def __getitem__(self, idx):\n","        sample = self.data[idx]\n","        return sample['image'], sample['bin_gt'], sample['seg_img']\n","    \n","    def __len__(self):\n","        return len(self.data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ds = LaneDataset(data_path)\n","ds1 = ds[0]"],"metadata":{"id":"Fs2qyheYZoyj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cv2_imshow(ds1[0])"],"metadata":{"id":"5AhWFU19750Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ds1[2].shape"],"metadata":{"id":"smpcz6K7-3DK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RFJKxiEESkDy"},"source":["For this task we are going to use ENet architecture. You can read the description in the [original paper](https://arxiv.org/pdf/1606.02147.pdf).\n","\n","To generate two different outputs we need to have two different branches. In the LaneNet paper authers proposed to share only two first sections of the encoder between to tasks. Third section and decoder should be separate for the instance segmentation and binary segmentation problem.\n","\n","The output dimension of the instance segmentation embedding should be equal to 5."]},{"cell_type":"code","metadata":{"id":"NNlg45XCSSOE"},"source":["class ENet(nn.Module):\n","    def __init__(self):\n","        super(ENet, self).__init__()\n","\n","    def forward(self, x):\n","      # TODO\n","      pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8RaJH5NrS9y5"},"source":["To obtain the instance segmentation we should train the embeddings as we described in the project description. In order to do that we need to implement [Discriminative Loss](https://arxiv.org/pdf/1708.02551.pdf). Discriminative loss consists of three parts:\n","\n","1. Inter-cluster push-force: cluster means should be far away from each other\n","2. Intra-cluster pull-force: embeddings should be close to the center of the corresponding cluster.\n","3. Regularization: cluster centers should be close to zero.\n","\n","Corresponding weights for the losses and other hyper-parameters could be found in the paper."]},{"cell_type":"code","metadata":{"id":"tvq18YyYTPfA"},"source":["class DiscriminativeLoss(torch.nn.modules.loss._Loss):\n","    def __init__(self, size_average=True):\n","        super(DiscriminativeLoss, self).__init__(size_average=size_average)\n","        # TODO\n","\n","    def forward(self, input, target):\n","        # TODO\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nnfigDZtk1IT"},"source":["Now we are ready to train the network. You may want to create validation subset to track metrics."]},{"cell_type":"code","metadata":{"id":"OWg5GUKWe9kV"},"source":["# TODO: Train segmentation and instance segmentation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6fbhm5VOs8W2"},"source":["## Homograpy Prediction Code"]},{"cell_type":"markdown","metadata":{"id":"AyoU1BX3tEaa"},"source":["Here we again need to implement the Dataset class. Dataset class should return resized image and ground truth points for the lane trajectories.\n"]},{"cell_type":"code","metadata":{"id":"oGAZAARrtD1j"},"source":["HNET_DEFAULT_SIZE = (64, 128)\n","import torch\n","\n","class HomographyPredictionDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataset_path, train=True, size=DEFAULT_SIZE):\n","        # TODO\n","        pass\n","\n","    def __getitem__(self, idx):\n","        # TODO\n","        return image, ground_truth_trajectory\n","    \n","    def __len__(self):\n","        # TODO\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5w_R4TxqugI8"},"source":["Now we need to implement the H-Net. It should return homography matrix that is parametrized by 6 numbers.\n","Corresponding section is CURVE FITTING USING H-NET in the[ LaneNet paper](https://arxiv.org/pdf/1802.05591.pdf).\n","\n","We suggest to follow the paper and take 64x128 image as in input.\n","\n","Homography projection contains discontinuities, therefore it could be quite difficult to train the network. In order to simplify the problem we propose not to train the homography from scratch, but train some correction to a good starting point.\n","\n","The following code block contains the initial homography that you can use.\n"]},{"cell_type":"code","metadata":{"id":"SCzbbkf4YbZd"},"source":["import torch\n","\n","# important note: this homography matrix is suitable only for 64x128 resolution\n","R = torch.Tensor([[-2.0484e-01, -1.7122e+01,  3.7991e+02],\n","                  [ 0.0000e+00, -1.6969e+01,  3.7068e+02],\n","                  [ 0.0000e+00, -4.6739e-02,  1.0000e+00]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UnoTHLy7Ym9Z"},"source":["Let's check the effect of this homography on the image."]},{"cell_type":"code","metadata":{"id":"25ibRC1dYXwE"},"source":["# !wget https://miro.medium.com/max/2560/1*b3HiwPg69Rw0L5iIMQMXOw.jpeg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_OkmlNSeZACe"},"source":["from matplotlib import pyplot as plt\n","import cv2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J6xkEFEgZFLG"},"source":["image = plt.imread('1*b3HiwPg69Rw0L5iIMQMXOw.jpeg')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vWdiK4J6arbT"},"source":["image = cv2.resize(image, (128, 64))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FSE2R6C3a1RI"},"source":["plt.imshow(image)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h_KUjfEwaGUI"},"source":["R_np = R.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRrAeXADZJpf"},"source":["plt.imshow(cv2.warpPerspective(image, R_np, dsize=(700, 700)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IxgEMTXjbFYL"},"source":["As you can see it looks like a bird's eye view projection where lines are becoming parallel."]},{"cell_type":"code","metadata":{"id":"rqsWLFv-ni5u"},"source":["class HNet(nn.Module):\n","    def __init__(self):\n","        super(HNet, self).__init__()\n","        # TODO\n","    def forward(self, x):\n","      # TODO\n","      pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KRcQ56HSu55-"},"source":["Finally, we need to implement the loss to fit the network. This is the most difficult part of the project. Homography loss should do the following:\n","\n","1. Project ground truth trajectory to the new plane according to the homography to obtain $(x', y')$.\n","2. Perform polynomial fitting $x' = f(y')$. Your code should work correctly with 2-nd and 3-rd order polynomials. Polynomial fitting should be done with the closed-form solution: $w = (Y^TY)^{-1}Y^Tx$ to be able to pass a gradient through. \n","3. Compute back projection with the inverse homography.\n","4. Compute loss between back projected $\\hat{x}$ and ground truth $x$.\n","\n","The full description can be found in the [LaneNet paper](https://arxiv.org/pdf/1802.05591.pdf).\n","\n","**IMPORTANT NOTES**\n","\n","* To fit the polynomial you need to compute the inverse matrix. Computation of the inverse is computation unstable operation. With the help of the SVD you can compute pseudo-inverse matrix: $(X^TX)^{-1}X^T$. In torch this function is called `torch.pinverse`.\n","* After projection use of the raw $y'$ can be impractical. The features for the polynomial fitting could have a high correlation and therefore solution can be unstable. At first, you need to map $y'$ to some fixed size segment and only then perform a fit.\n","* After projection some points can occur on the other side of image (bottom part of the projection in the example above). You may want to exclude them, because otherwise polynomial is not going to have much sense and gradients can have a dramatically large values. Positive $w$ in homogeneous coordinates indicates that point is on the wrong side.\n","* If use the homography provided above, your ground truth should have the same scale, as an image\n","* Every division in your network is a potential place to obtain `nan` in gradients. Use the following trick to obtain less divisions:\n","after computing the projection, remember the $w$. Then, before the back projection, multiply your vector by $w$. After back projection you will see that $w$ is equal to 1. So you don't need to devide your vector by $w$.\n","* You correction to the initial homography could be very big and can completely spoil the homography. So you may need to scale the output of the last layer in the HNet to obtain a smaller corrections.\n"]},{"cell_type":"code","metadata":{"id":"RImq1jJ9u5bP"},"source":["class HomographyLoss(torch.nn.modules.loss._Loss):\n","    def __init__(self, size_average=True):\n","        super(HomographyLoss, self).__init__(size_average=size_average)\n","\n","    def forward(self, input, target):\n","        # TODO\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E5Nm6iDmg7Ds"},"source":["Finally, train the homography net with the 2-nd order polynomial fitting (you can 3-rd order but we found out that it is much easier to train h-net with the 2-nd order polynomials).\n","\n","We found out that SGD fits much better for this architectures. Adam (and momentums inside) can ruin the weights because of the discontinuities in the target function surface.\n","\n","Demonstrate the difference in homography loss between h-net and provided fixed homography on the validation samples."]},{"cell_type":"code","metadata":{"id":"Nh5tvQZFg6Wz"},"source":["# Train the H-Net"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e06hUkw3lxJH"},"source":["# Complete Pipeline\n","\n","Now you need to implement a class that takes image and a set of $y$ coordinates as an input and returns the $x$ coordinate for each lane in this image.\n","\n","It should contain the following steps:\n","\n","1. Apply segmentation network to the image\n","2. Run DBSCAN over the embeddings for those pixels that are lanes.\n","3. Apply h-net to the image\n","4. Project pixel coordinates with the predicted homograpgy\n","5. Fit the 3-rd order polynomial\n","6. Predict the lane position for each provided $y$ (you should project this first).\n","7. Compute back projection and return the positions of $x$ for each lane.\n","\n","Note: if you weren't able to train the H-Net, you can use the fixed homography here."]},{"cell_type":"code","metadata":{"id":"uZ-i3Nl9l0zV"},"source":["class LaneDetector:\n","  def __init__(self, hnet, lanenet):\n","    pass\n","  \n","  def __call__(self, image, y_positions):\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Su4pCsmlwV7"},"source":[],"execution_count":null,"outputs":[]}]}